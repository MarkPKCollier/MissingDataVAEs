{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Perform common imports and data setup which is shared across notebooks.\n",
    "\n",
    "**Note:** this notebook should be run as a part of another notebook using `%run` magic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings are imported first to filter out import warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error:  This version of TensorFlow Probability requires TensorFlow version >= 1.12.0; Detected an installation of version 1.9.0. Please upgrade TensorFlow to proceed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import base64\n",
    "    import boto3\n",
    "    import csv\n",
    "    import ftfy\n",
    "    import gensim\n",
    "    import hashlib\n",
    "    import hdbscan\n",
    "    import html\n",
    "    import io\n",
    "    import itertools\n",
    "    import json\n",
    "    import json_lines\n",
    "    import langid\n",
    "    import logging\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mpld3\n",
    "    import multiprocessing\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "    import operator\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import pprint as pp\n",
    "    import pyarrow.parquet as pq\n",
    "    import pytextrank\n",
    "    import random\n",
    "    import re\n",
    "    import requests\n",
    "    import scipy\n",
    "    import seaborn as sns\n",
    "    import shutil\n",
    "    import six\n",
    "    import smart_open\n",
    "    import string\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_probability as tfp\n",
    "    import time\n",
    "    import traceback\n",
    "    import urllib\n",
    "    import xml.etree.ElementTree as etree\n",
    "\n",
    "\n",
    "    from bokeh.io import output_notebook, show\n",
    "    from bokeh.models import ColumnDataSource, HoverTool\n",
    "    from bokeh.plotting import figure\n",
    "    from botocore.exceptions import ClientError\n",
    "    from bs4 import BeautifulSoup\n",
    "    from collections import Counter, defaultdict, namedtuple\n",
    "    from datetime import datetime\n",
    "    from enum import Enum\n",
    "    from functools import reduce\n",
    "    from glob import glob\n",
    "    from io import BytesIO, StringIO\n",
    "    from IPython.display import display, HTML, Image\n",
    "#     from Levenshtein import distance as levenshtein_distance\n",
    "    from matplotlib import gridspec, rc\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "    from newspaper import Article\n",
    "    from nltk import pos_tag\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from pathlib import Path\n",
    "    from PIL import Image as pil_img\n",
    "    from pprint import pprint\n",
    "    from scipy import spatial\n",
    "    from scipy.cluster.hierarchy import ward, dendrogram\n",
    "    from shutil import copyfile\n",
    "    from sklearn import datasets, decomposition, feature_extraction, metrics, model_selection, preprocessing\n",
    "    from sklearn.cluster import KMeans, DBSCAN\n",
    "    from sklearn.decomposition import PCA, TruncatedSVD\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.manifold import MDS\n",
    "    from sklearn.metrics import accuracy_score, average_precision_score, auc, f1_score, precision_score, precision_recall_curve, recall_score, roc_auc_score, roc_curve\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.model_selection import KFold, train_test_split\n",
    "    from sklearn.utils import shuffle\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Categorical, Integer, Real\n",
    "#     from skopt.utils import use_named_args\n",
    "    from stringcase import camelcase, snakecase\n",
    "    from tensor2tensor import models, problems\n",
    "    from tensor2tensor.data_generators import problem, text_problems\n",
    "    from tensor2tensor.utils import registry, trainer_lib\n",
    "    from tensorflow.contrib.rnn import GRUCell, LSTMCell, LSTMStateTuple\n",
    "    from tensorflow.python.feature_column import feature_column as fc_core\n",
    "    from tensorflow.python.framework import ops\n",
    "    from tensorflow.python.saved_model import constants\n",
    "    from termcolor import colored, cprint\n",
    "    from textblob import TextBlob as tb\n",
    "    from tqdm import tqdm_notebook\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.request import urlopen\n",
    "except ImportError as e:\n",
    "    print('Import error: ', e)\n",
    "\n",
    "Modes = tf.estimator.ModeKeys\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Eager\n",
    "Optionally enable  mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TF_EAGER\n",
    "except:\n",
    "    TF_EAGER = False\n",
    "if TF_EAGER:\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupiter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 30\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "# trainer_lib.set_random_seed(42)\n",
    "print('Notebook & runtime settings applied...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_directory(dir_path, rmdir=False):\n",
    "    \"\"\"Creates a directory if it doesn't exist; otherwise clears its contents.\"\"\"\n",
    "    if os.path.exists(dir_path) and rmdir:\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    return dir_path\n",
    "\n",
    "def get_n_grams(word_list, n=2):\n",
    "    \"\"\"Compute ngrams for a list of words.\"\"\"\n",
    "    if n < 2:\n",
    "#         raise ValueError('Please provide n which is >= 2.')\n",
    "        warnings.warn('Obtaining ngrams with n < 2.', Warning)\n",
    "        return word_list\n",
    "    \n",
    "    n_grams = []\n",
    "    for idx, w in enumerate(word_list[:-(n - 1)]):\n",
    "        subsequent_words = [word_list[i] for i in range(idx + 1, idx + 1 + (n - 1))]\n",
    "        n_grams.append('_'.join([w] + subsequent_words))\n",
    "    return n_grams\n",
    "\n",
    "def get_sentence_n_grams(sentence_word_list, n=2):\n",
    "    \"\"\"Compute ngrams for an array of sentences (word lists).\"\"\"\n",
    "    ngrams = []\n",
    "    for s in sentence_word_list:\n",
    "        ngrams += get_n_grams(s, n)\n",
    "    return ngrams\n",
    "\n",
    "def get_tf_idf_dict_for_text(text_str, vectorizer, tf_idf_words):\n",
    "    \"\"\"Get TF-IDF results for a text (returns word:score dictionary).\"\"\"\n",
    "    tf_idf_dict = {x:0 for x in text_str.split(' ')}\n",
    "    transform_result = vectorizer.transform([text_str])\n",
    "    coo = transform_result.tocoo()\n",
    "    non_zero_scores = {tf_idf_words[term_id]: score for doc_id, term_id, score in\n",
    "                       zip(coo.row, coo.col, coo.data) if score > 0}\n",
    "    tf_idf_dict.update(non_zero_scores)\n",
    "    return tf_idf_dict\n",
    "\n",
    "def strip_text(text):\n",
    "    \"\"\"Leave only allowed set of characters in a text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9?.!,Â¿\\-\\'\"()]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def get_article_for_url(url):\n",
    "    \"\"\"Returns parsed newspaper3k article from URL.\"\"\"\n",
    "    a = Article(url=url, fetch_images=False, request_timeout=5)\n",
    "    a.download()\n",
    "    a.parse()\n",
    "    return a\n",
    "\n",
    "def get_html_for_url(url):\n",
    "    \"\"\"Returns HTML from URL (extracted via newspaper3k).\"\"\"\n",
    "    try:\n",
    "        t = get_article_for_url(url).html\n",
    "    except:\n",
    "        t = ''\n",
    "    return t\n",
    "\n",
    "def get_text_for_url(url):\n",
    "    \"\"\"Returns text from URL (extracted via newspaper3k).\"\"\"\n",
    "    try:\n",
    "        t = get_article_for_url(url).text\n",
    "    except:\n",
    "        t = ''\n",
    "    return t\n",
    "\n",
    "def find_all_substr(source_str, target_str):\n",
    "    \"\"\"Find all occurances of `target_str` in `source_str`.\"\"\"\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = source_str.find(target_str, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(target_str) # use start += 1 to find overlapping matches\n",
    "        \n",
    "def find_all_sublists(source_list, target_list):\n",
    "    \"\"\"Find all occurances of `target_list` in `source_list`.\"\"\"\n",
    "    start = 0\n",
    "    target_len = len(target_list)\n",
    "    indices = []\n",
    "    for i in range(start, len(source_list) - len(target_list) + 1):\n",
    "        if source_list[i:i+target_len] == target_list:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_host_from_url(url):\n",
    "    \"\"\"Returns domain from URL.\"\"\"\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "def get_md5_hash_str(input_str):\n",
    "    \"\"\"Gets an MD5 hash string from input string.\"\"\"\n",
    "    return hashlib.md5(input_str.encode('utf8')).hexdigest()\n",
    "\n",
    "def flatten_2d_list(list_2d):\n",
    "    \"\"\"Make a flat (1-d) list out of 2-d list (list of lists).\"\"\"\n",
    "    if list_2d is None:\n",
    "        return list_2d\n",
    "    return list(itertools.chain.from_iterable(list_2d))\n",
    "\n",
    "def get_language(input_str, max_str_length=300):\n",
    "    \"\"\"Returns the language of a string.\"\"\"\n",
    "    return langid.classify(input_str[:max_str_length])[0]\n",
    "\n",
    "def safe_str_to_int_cast(input_str):\n",
    "    try:\n",
    "        return int(input_str)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "LINK_REGEX = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)'\n",
    "def extract_links_from_text(text):\n",
    "    \"\"\"Extracts HTTP(S) links from input text.\"\"\"\n",
    "    matches = re.finditer(LINK_REGEX, text)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def strip_expr(source_str, strip_expr):\n",
    "    \"\"\"Strips the `strip_expr` from the end of a string if it is present.\"\"\"\n",
    "    return source_str[:-len(strip_expr)] if source_str[-len(strip_expr):] == strip_expr else source_str\n",
    "\n",
    "def strip_exprs(source_str, strip_exprs_list):\n",
    "    \"\"\"Strips any of the `strip_exprs` from the end of a string.\"\"\"\n",
    "    for expr in strip_exprs_list:\n",
    "        if source_str[-len(expr):] == expr:\n",
    "            source_str = source_str[:-len(expr)]\n",
    "    return source_str\n",
    "\n",
    "def get_file_logger(log_file_path='custom_log.log', logger_name='custom_logger'):\n",
    "    \"\"\"Returns a logger which logs to a file with datetime-formatted messages.\"\"\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.FileHandler(log_file_path)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(levelname)s:%(asctime)s:%(message)s', datefmt='%m/%d/%Y %I:%M:%S')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    logger.info('Log file created')\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Utils\n",
    "#### Logging Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return not record.getMessage().startswith('Initialize variable')\n",
    "\n",
    "tf_logger = logging.getLogger('tensorflow')\n",
    "tf_logger.addFilter(TFFilter())\n",
    "print('TensorFlow logging filter applied...')\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights_from_ckpt(source_ckpt, target_ckpt):\n",
    "    \"\"\"Copies all weights with matching names from `source_ckpt` to `target_ckpt`.\"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Copy all variables which are currently in a target checkpoint.\n",
    "    target_reader = tf.train.NewCheckpointReader(target_ckpt)\n",
    "    source_reader = tf.train.NewCheckpointReader(source_ckpt)\n",
    "\n",
    "    target_vars = target_reader.get_variable_to_shape_map()\n",
    "    source_vars = source_reader.get_variable_to_shape_map()\n",
    "    \n",
    "    checkpoint_vars = {}\n",
    "    for name in target_reader.get_variable_to_shape_map():\n",
    "        if name in source_vars:\n",
    "            checkpoint_vars[name] = tf.Variable(source_reader.get_tensor(name))\n",
    "        else:\n",
    "            checkpoint_vars[name] = tf.Variable(target_reader.get_tensor(name))\n",
    "            \n",
    "    # Save updated checkpoint file.\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(checkpoint_vars)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.save(sess, target_ckpt)\n",
    "        \n",
    "        \n",
    "def rename_vars_in_ckpt(name_map, in_file, out_file):\n",
    "    \"\"\"Renames variables in checkpoint according to a mapping scheme.\n",
    "     \n",
    "    Adopted from: https://github.com/KranthiGV/Pretrained-Show-and-Tell-model/issues/7#issuecomment-309862894\n",
    "     \n",
    "    Example of variable renaming:\n",
    "    OLD_CHECKPOINT_FILE = os.path.join(MODEL_PATH, 'checkpoint-329')\n",
    "    NEW_CHECKPOINT_FILE = os.path.join(MODEL_PATH, 'checkpoint-329-fixed')\n",
    "    vars_to_rename = {\n",
    "        'lstm/basic_lstm_cell/weights': 'lstm/basic_lstm_cell/kernel',\n",
    "        'lstm/basic_lstm_cell/biases': 'lstm/basic_lstm_cell/bias',\n",
    "    }\n",
    "    rename_vars_in_ckpt(vars_to_rename, OLD_CHECKPOINT_FILE, NEW_CHECKPOINT_FILE)\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    new_checkpoint_vars = {}\n",
    "    reader = tf.train.NewCheckpointReader(in_file)\n",
    "    for old_name in reader.get_variable_to_shape_map():\n",
    "        if old_name in name_map:\n",
    "            new_name = name_map[old_name]\n",
    "        else:\n",
    "            new_name = old_name\n",
    "        new_checkpoint_vars[new_name] = tf.Variable(reader.get_tensor(old_name))\n",
    " \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(new_checkpoint_vars)\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.save(sess, out_file)\n",
    "        \n",
    "        \n",
    "def average_checkpoints(checkpoints='',\n",
    "                        num_last_checkpoints=0,\n",
    "                        prefix='',\n",
    "                        output_path='/tmp/averaged.ckpt'):\n",
    "    \"\"\"Creates a TF checkpoint which is an average of provided checkpoints.\n",
    "    \n",
    "    Adopted from https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/avg_checkpoints.py.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints: Comma-separated list of checkpoints to average.\n",
    "        num_last_checkpoints: Averages the last N saved checkpoints.\n",
    "            If the checkpoints flag is set, this is ignored.\n",
    "        prefix: Path to output the averaged checkpoint to.\n",
    "    \"\"\"\n",
    "    if checkpoints:\n",
    "        # Get the checkpoints list and run some basic checks.\n",
    "        checkpoints = [c.strip() for c in checkpoints.split(',')]\n",
    "        checkpoints = [c for c in checkpoints if c]\n",
    "        if not checkpoints:\n",
    "            raise ValueError('No checkpoints provided for averaging.')\n",
    "        if prefix:\n",
    "            checkpoints = [prefix + c for c in checkpoints]\n",
    "    else:\n",
    "        assert num_last_checkpoints >= 1, 'Must average at least one model'\n",
    "        assert prefix, ('Prefix must be provided when averaging last'\n",
    "                        ' N checkpoints')\n",
    "        checkpoint_state = tf.train.get_checkpoint_state(\n",
    "            os.path.dirname(prefix))\n",
    "        # Checkpoints are ordered from oldest to newest.\n",
    "        checkpoints = checkpoint_state.all_model_checkpoint_paths[\n",
    "            -num_last_checkpoints:]\n",
    "\n",
    "    if not checkpoints:\n",
    "        if checkpoints:\n",
    "            raise ValueError(\n",
    "                f'None of the provided checkpoints exist. {checkpoints}')\n",
    "        else:\n",
    "            raise ValueError(f'Could not find checkpoints at {os.path.dirname(prefix)}')\n",
    "            \n",
    "    # Read variables from all checkpoints and average them.\n",
    "    tf.logging.info('Reading variables and averaging checkpoints:')\n",
    "    for c in checkpoints:\n",
    "        tf.logging.info(f'{c} ')\n",
    "    var_list = tf.contrib.framework.list_variables(checkpoints[0])\n",
    "    var_values, var_dtypes = {}, {}\n",
    "    for (name, shape) in var_list:\n",
    "        if not name.startswith('global_step'):\n",
    "            var_values[name] = np.zeros(shape)\n",
    "    for checkpoint in checkpoints:\n",
    "        reader = tf.contrib.framework.load_checkpoint(checkpoint)\n",
    "        for name in var_values:\n",
    "            tensor = reader.get_tensor(name)\n",
    "            var_dtypes[name] = tensor.dtype\n",
    "            var_values[name] += tensor\n",
    "        tf.logging.info(f'Read from checkpoint {checkpoint}')\n",
    "    for name in var_values:  # Average.\n",
    "        var_values[name] /= len(checkpoints)\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "        tf_vars = [\n",
    "            tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v]) \\\n",
    "            for v in var_values]\n",
    "    placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n",
    "    assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64)\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build a model consisting only of variables, set them to the average values.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for p, assign_op, (name, value) in zip(\n",
    "            placeholders, assign_ops, six.iteritems(var_values)):\n",
    "            sess.run(assign_op, {p: value})\n",
    "        # Use the built saver to save the averaged checkpoint.\n",
    "        saver.save(sess, output_path, global_step=global_step)\n",
    "    tf.logging.info(f'Averaged checkpoints saved in {output_path}')\n",
    "    \n",
    "    # Copy graph file.\n",
    "    shutil.copy2(os.path.join(os.path.dirname(prefix), 'graph.pbtxt'), os.path.dirname(output_path))\n",
    "    tf.logging.info(f'Copied graph.pbtxt file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_CNT = 25\n",
    "NOISE_REDUCTION_LEVEL = 1e4\n",
    "MAX_CATEGORICAL_FEATURES = 7\n",
    "MAX_BIN_TICKS = 7\n",
    "\n",
    "def plot_bins_numerical(df, feature_names, class_column, class_values, class_colors=None, feature_bin_cnts=None, plot_kde=False):\n",
    "    \"\"\"Plots histograms for numerical features.\"\"\"\n",
    "    fig = plt.figure(figsize=(14, int(len(feature_names)) * 1.5))\n",
    "    gs = gridspec.GridSpec(math.ceil(len(feature_names) / 3), 3)\n",
    "    df = df[feature_names + [class_column]].dropna()\n",
    "\n",
    "    # Number of bins for each feature. If counts are not provided will use BIN_CNT bins.\n",
    "    feature_bin_cnts = feature_bin_cnts if feature_bin_cnts else [BIN_CNT] * len(feature_names)\n",
    "    \n",
    "    # Use default colors if class colors are not provided.\n",
    "    if not class_colors:\n",
    "        cmap = plt.cm.get_cmap('hsv', len(class_values))\n",
    "        class_colors = list(map(cmap, range(len(class_values))))\n",
    "    \n",
    "    # Add some noise so that bin edges are unique.\n",
    "    df_numerical = df[feature_names].copy()\n",
    "    df_numerical += np.random.random(df_numerical.shape) / NOISE_REDUCTION_LEVEL\n",
    "    \n",
    "    for i, (f, bin_cnt) in enumerate(zip(feature_names, feature_bin_cnts)):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        ax.set_xlabel('Bin Number')\n",
    "        if (bin_cnt > MAX_BIN_TICKS) or plot_kde:\n",
    "            ax.set_xticks(range(0, bin_cnt + 1, bin_cnt // MAX_BIN_TICKS))\n",
    "        else:\n",
    "            ax.set_xticks(range(bin_cnt + 1))\n",
    "        ax.set_ylabel('Kernel Density')\n",
    "        ax.set_title(f)\n",
    "        \n",
    "        # Split data in `bin_cnt` buckets of equal (by number of examples) size.\n",
    "        bins = pd.qcut(df_numerical[f], bin_cnt, labels=False)\n",
    "\n",
    "        # Plot distribution of examples into bins for each of the classes.\n",
    "        for class_name, class_color in zip(class_values, class_colors):\n",
    "            bins_s = bins[df[class_column] == class_name]\n",
    "            \n",
    "            if not len(bins_s):\n",
    "                continue\n",
    "                \n",
    "            if plot_kde:\n",
    "                sns.kdeplot(bins_s.values,\n",
    "                            shade=True,\n",
    "                            cut=5,\n",
    "                            color=class_color,\n",
    "                            label=class_name)\n",
    "            else:\n",
    "                sns.distplot(bins_s.values,\n",
    "                             bins=bin_cnt,\n",
    "                             color=class_color,\n",
    "                             kde=False,\n",
    "                             norm_hist=True,\n",
    "                             hist_kws=dict(edgecolor='dimgrey', linewidth=1),\n",
    "                             label=class_name)\n",
    "                plt.legend()\n",
    "            \n",
    "    gs.tight_layout(fig)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bins_categorical(df, feature_names, class_column, class_values, class_colors=None, max_category_cnt=7):\n",
    "    \"\"\"Plots histograms for categorical features.\n",
    "    \n",
    "    It takes up to `max_category_cnt` top categories across all classes.\n",
    "    Then for each class it plots (values[category == given_category] / values).\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, len(feature_names)))\n",
    "    gs = gridspec.GridSpec(math.ceil(len(feature_names) / 3), 3)\n",
    "\n",
    "    # Use default colors if class colors are not provided.\n",
    "    if not class_colors:\n",
    "        cmap = plt.cm.get_cmap('RdYlGn', len(class_values))\n",
    "        class_colors = cmap(range(len(class_values)))\n",
    "        \n",
    "    for i, f in enumerate(feature_names):\n",
    "        # Get top categories for the current feature.\n",
    "        top_c = list(df[f].value_counts()[:max_category_cnt].index)\n",
    "        df_c = df[df[f].isin(top_c)][[class_column, f]]\n",
    "        \n",
    "        # Get % which values of this category take of all values.\n",
    "        value_cnt_pct, categories, classes = [], [], []\n",
    "        for l in df[class_column].unique():\n",
    "            v_c = pd.DataFrame(df_c[df_c[class_column] == l][f].value_counts())\n",
    "            v_c['pct'] = v_c[f] / v_c[f].sum()\n",
    "            value_cnt_pct.extend(list(v_c['pct']))\n",
    "            categories.extend(list(v_c.index))\n",
    "            classes.extend([l] * len(v_c))\n",
    "        \n",
    "        categories_df = pd.DataFrame({'value_cnt_pct': value_cnt_pct,\n",
    "                                      'category': categories,\n",
    "                                      'class': classes})\n",
    "\n",
    "        # Plot. Heights of bars for each class sum up to one.\n",
    "        # Note: having heights of bars within category sum up to one may be\n",
    "        # misleading in cases of class imbalance.\n",
    "        ax = plt.subplot(gs[i])\n",
    "        sns.barplot(ax=ax, x='category', y='value_cnt_pct', hue='class', data=categories_df,\n",
    "                    palette=class_colors, alpha=0.5, saturation=0.9, edgecolor='dimgrey', linewidth=1)\n",
    "        ax.set_title(f)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('% of Total')\n",
    "\n",
    "\n",
    "    gs.tight_layout(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualisation Functions\n",
    "Credit: http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:1000px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1000px;height:1000px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "    \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "\n",
    "\n",
    "def get_feature_dict(features):\n",
    "    if isinstance(features, dict):\n",
    "        return features\n",
    "    return {'': features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shampoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"The Shampoo Optimizer.\n",
    "\n",
    "Variant of Adagrad using one preconditioner matrix per variable dimension.\n",
    "For details, see https://arxiv.org/abs/1802.09568\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import linalg_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.platform import tf_logging\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "\n",
    "def GetParam(var, timestep):\n",
    "  if callable(var):\n",
    "    return var(timestep)\n",
    "  else:\n",
    "    return var\n",
    "\n",
    "\n",
    "class ShampooOptimizer(optimizer.Optimizer):\n",
    "  \"\"\"The Shampoo Optimizer\n",
    "\n",
    "  Variant of Adagrad using one preconditioner matrix per variable dimension.\n",
    "  For details, see https://arxiv.org/abs/1802.09568\n",
    "\n",
    "  gbar is time-weighted accumulated gradient:\n",
    "  gbar[t] = gbar_decay[t] * gbar[t-1] + gbar_weight[t] * g[t]\n",
    "\n",
    "  mat_gbar is time-weighted accumulated gradient square:\n",
    "  mat_gbar_j[t] = mat_gbar_decay[t] * mat_gbar_j[t-1]\n",
    "                  + mat_gbar_weight[t] * gg_j[t]\n",
    "  where if g[t] = g_abcd then gg_a[t] = g_abcd g_a'bcd (Einstein notation)\n",
    "\n",
    "  Update rule:\n",
    "  w[t+1] = w[t] - learning_rate[t] * Prod_j mat_gbar_j[t]^(-alpha/n) gbar[t]\n",
    "     Again, mat_gbar_j[t]^(-alpha) gbar[t] is a tensor contraction along the\n",
    "     j'th dimension of gbar[t] with the first dimension of\n",
    "     mat_gbar_j[t]^(-alpha/n), where alpha is a hyperparameter,\n",
    "     and n = rank of the variable.\n",
    "     Prod_j represents doing this contraction for all j in 0..n-1.\n",
    "\n",
    "  Typically learning_rate is constant, but could be time dependent by passing\n",
    "  a lambda function that depends on step.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               global_step=0,\n",
    "               max_matrix_size=768,\n",
    "               gbar_decay=0.0,\n",
    "               gbar_weight=1.0,\n",
    "               mat_gbar_decay=1.0,\n",
    "               mat_gbar_weight=1.0,\n",
    "               learning_rate=1.0,\n",
    "               svd_interval=1,\n",
    "               precond_update_interval=1,\n",
    "               epsilon=0.1,\n",
    "               alpha=0.5,\n",
    "               use_iterative_root=False,\n",
    "               use_locking=False,\n",
    "               name=\"Shampoo\"):\n",
    "    \"\"\"Default values of the various hyper-parameters.\n",
    "\n",
    "    gbar_decay, gbar_weight etc. can be a float or a time varying parameter.\n",
    "    For time-varying parameters use e.g. \"lambda T: T / (T + 1.0)\"\n",
    "    where the expression in the lambda is a tensorflow expression\n",
    "\n",
    "    Args:\n",
    "      global_step: tensorflow variable indicating the step.\n",
    "      max_matrix_size: We do not perform SVD for matrices larger than this.\n",
    "      gbar_decay:\n",
    "      gbar_weight:  Used to update gbar:\n",
    "            gbar[t] = gbar_decay[t] * gbar[t-1] + gbar_weight[t] * g[t]\n",
    "      mat_gbar_decay:\n",
    "      mat_gbar_weight:  Used to update mat_gbar:\n",
    "           mat_gbar_j[t] = mat_gbar_decay[t] * mat_gbar_j[t-1]\n",
    "                           + mat_gbar_weight[t] * gg_j[t]\n",
    "      learning_rate: Similar to SGD\n",
    "      svd_interval: We should do SVD after this many steps. Default = 1, i.e.\n",
    "                    every step. Usually 20 leads to no loss of accuracy, and\n",
    "                    50 or 100 is also OK. May also want more often early,\n",
    "                    and less often later - set in caller as for example:\n",
    "                    \"svd_interval = lambda(T): tf.cond(\n",
    "                        T < 2000, lambda: 20.0, lambda: 1000.0)\"\n",
    "      precond_update_interval: We should update the preconditioners after\n",
    "                               this many steps. Default = 1. Usually less than\n",
    "                               svd_interval.\n",
    "      epsilon:  epsilon * I_n is added to each mat_gbar_j for stability\n",
    "      alpha:  total power of the preconditioners.\n",
    "      use_iterative_root: should the optimizer use SVD (faster) or the\n",
    "                          iterative root method (for TPU) for finding the\n",
    "                          roots of PSD matrices.\n",
    "      use_locking:\n",
    "      name: name of optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    super(ShampooOptimizer, self).__init__(use_locking, name)\n",
    "\n",
    "    self._global_step = math_ops.to_float(global_step)\n",
    "    self._max_matrix_size = max_matrix_size\n",
    "    self._gbar_decay = gbar_decay\n",
    "    self._gbar_weight = gbar_weight\n",
    "    self._mat_gbar_decay = mat_gbar_decay\n",
    "    self._mat_gbar_weight = mat_gbar_weight\n",
    "    self._learning_rate = learning_rate\n",
    "    self._svd_interval = svd_interval\n",
    "    self._precond_update_interval = precond_update_interval\n",
    "    self._epsilon = epsilon\n",
    "    self._alpha = alpha\n",
    "    self._use_iterative_root = use_iterative_root\n",
    "    self._name = name\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      with ops.colocate_with(v):\n",
    "        _ = self._zeros_slot(v, \"gbar\", self._name)\n",
    "        shape = np.array(v.get_shape())\n",
    "        for i, d in enumerate(shape):\n",
    "          d_tensor = ops.convert_to_tensor(d)\n",
    "          if d <= self._max_matrix_size:\n",
    "            mat_g_init = array_ops.zeros_like(linalg_ops.eye(d_tensor))\n",
    "            if self._svd_interval > 1:\n",
    "              _ = self._get_or_make_slot(v, linalg_ops.eye(d_tensor),\n",
    "                                         \"H_\" + str(i), self._name)\n",
    "          else:\n",
    "            mat_g_init = array_ops.zeros([d_tensor])\n",
    "\n",
    "          _ = self._get_or_make_slot(v, mat_g_init, \"Gbar_\" + str(i),\n",
    "                                     self._name)\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var):\n",
    "    return self._apply_dense(grad, var)\n",
    "\n",
    "  def _apply_dense(self, grad, var):\n",
    "    return self._apply_gradient(grad, var)\n",
    "\n",
    "  def _resource_apply_sparse(self, grad_values, var, grad_indices):\n",
    "    return self._apply_sparse_shared(grad_values, grad_indices, var)\n",
    "\n",
    "  def _apply_sparse(self, grad, var):\n",
    "    return self._apply_sparse_shared(grad.values, grad.indices, var)\n",
    "\n",
    "  def _apply_sparse_shared(self, grad_values, grad_indices, var):\n",
    "    if var.get_shape()[0] <= self._max_matrix_size or self._gbar_decay != 0.0:\n",
    "      # The dimension is small enough, we can make the variable dense and\n",
    "      # do a dense update\n",
    "      dense_grad = array_ops.scatter_nd(\n",
    "          array_ops.expand_dims(grad_indices, axis=1), grad_values,\n",
    "          array_ops.shape(var, out_type=grad_indices.dtype))\n",
    "      return self._apply_gradient(dense_grad, var)\n",
    "    return self._apply_gradient(grad_values, var, grad_indices)\n",
    "\n",
    "  def _weighted_average(self, var, weight, weight_t, rest):\n",
    "    \"\"\"Computes exponential weighted average: var = weight_t * var + rest.\n",
    "\n",
    "    Important to ensure that var does not occur in rest, otherwise\n",
    "    we can get race conditions in a distributed setting.\n",
    "\n",
    "    Args:\n",
    "      var: variable to be updated\n",
    "      weight: parameter to be checked. If it is a constant, we can optimize.\n",
    "      weight_t: current value of parameter, used for weighting\n",
    "      rest: the remaining tensor to be added\n",
    "\n",
    "    Returns:\n",
    "      updated variable.\n",
    "    \"\"\"\n",
    "    if weight == 0.0:\n",
    "      return rest       # no need to update var, we will never use it.\n",
    "    if weight == 1.0:   # common case\n",
    "      return state_ops.assign_add(var, rest)\n",
    "    # The op below can cause race conditions in a distributed setting,\n",
    "    # since computing weight_t * var + rest can take some time, during\n",
    "    # which var may be set by another worker. To prevent this, it should\n",
    "    # be implemented as a C++ op.\n",
    "    return var.assign_add((weight_t - 1) * var + rest)\n",
    "\n",
    "  def _update_mat_g(self, mat_g, grad, axes, mat_gbar_decay,\n",
    "                    mat_gbar_weight, i):\n",
    "    \"\"\"Updates the cumulative outer products of the gradients.\n",
    "\n",
    "    Args:\n",
    "      mat_g: the matrix to be updated\n",
    "      grad: the gradient of the variable\n",
    "      axes: a list of k-1 integers 0 to k-1, except i\n",
    "      mat_gbar_decay: constant for weighted average:\n",
    "          mat_g = mat_g * decay + grad * weight\n",
    "      mat_gbar_weight: constant for weighted average\n",
    "      i: index of dimension to be updated.\n",
    "\n",
    "    Returns:\n",
    "      updated mat_g = mat_g * mat_gbar_decay + grad_outer * mat_gbar_weight\n",
    "\n",
    "    In Einstein notation if i = 0: grad_outer_aa'= g_abcd g_a'bcd\n",
    "    thus grad_outer is a matrix d_i x d_i, where d_i is the size of the\n",
    "    i'th dimension of g.\n",
    "    Alternate view: If mat_i(grad) is the flattening of grad to a\n",
    "    d_i x (d_1d_2...d_{i-1}d_{i+1}...d_k) matrix, then\n",
    "         grad_outer = mat_i(grad) mat_i(grad).transpose\n",
    "    \"\"\"\n",
    "    grad_outer = math_ops.tensordot(grad, grad, axes=(axes, axes),\n",
    "                                    name=\"grad_outer_\" + str(i))\n",
    "    return self._weighted_average(mat_g, self._mat_gbar_decay, mat_gbar_decay,\n",
    "                                  mat_gbar_weight * grad_outer)\n",
    "\n",
    "  def _compute_power_svd(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name):\n",
    "    \"\"\"Computes mat_h = mat_g^alpha using svd. mat_g is a symmetric PSD matrix.\n",
    "\n",
    "    Args:\n",
    "      var: the variable we are updating.\n",
    "      mat_g: the symmetric PSD matrix whose power it to be computed\n",
    "      mat_g_size: size of mat_g\n",
    "      alpha: a real number\n",
    "      mat_h_slot_name: name of slot to store the power, if needed.\n",
    "\n",
    "    Returns:\n",
    "      mat_h = mat_g^alpha\n",
    "\n",
    "    Stores mat_h in the appropriate slot, if it exists.\n",
    "    Note that mat_g is PSD. So we could use linalg_ops.self_adjoint_eig.\n",
    "    \"\"\"\n",
    "    if mat_g_size == 1:\n",
    "      mat_h = math_ops.pow(mat_g + self._epsilon, alpha)\n",
    "    else:\n",
    "      damping = self._epsilon * linalg_ops.eye(math_ops.to_int32(mat_g_size))\n",
    "      diag_d, mat_u, mat_v = linalg_ops.svd(mat_g + damping, full_matrices=True)\n",
    "      mat_h = math_ops.matmul(\n",
    "          mat_v * math_ops.pow(math_ops.maximum(diag_d, self._epsilon), alpha),\n",
    "          array_ops.transpose(mat_u))\n",
    "    if mat_h_slot_name is not None:\n",
    "      return state_ops.assign(self.get_slot(var, mat_h_slot_name), mat_h)\n",
    "    return mat_h\n",
    "\n",
    "  def _compute_power_iter(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name,\n",
    "                          iter_count=100, epsilon=1e-6):\n",
    "    \"\"\"Computes mat_g^alpha, where alpha = -1/p, p a positive integer.\n",
    "\n",
    "    We use an iterative Schur-Newton method from equation 3.2 on page 9 of:\n",
    "\n",
    "    A Schur-Newton Method for the Matrix p-th Root and its Inverse\n",
    "    by Chun-Hua Guo and Nicholas J. Higham\n",
    "    SIAM Journal on Matrix Analysis and Applications,\n",
    "    2006, Vol. 28, No. 3 : pp. 788-804\n",
    "    https://pdfs.semanticscholar.org/0abe/7f77433cf5908bfe2b79aa91af881da83858.pdf\n",
    "\n",
    "    Args:\n",
    "      var: the variable we are updating.\n",
    "      mat_g: the symmetric PSD matrix whose power it to be computed\n",
    "      mat_g_size: size of mat_g.\n",
    "      alpha: exponent, must be -1/p for p a positive integer.\n",
    "      mat_h_slot_name: name of slot to store the power, if needed.\n",
    "      iter_count: Maximum number of iterations.\n",
    "      epsilon: accuracy indicator, useful for early termination.\n",
    "\n",
    "    Returns:\n",
    "      mat_g^alpha\n",
    "    \"\"\"\n",
    "\n",
    "    identity = linalg_ops.eye(math_ops.to_int32(mat_g_size))\n",
    "\n",
    "    def MatPower(mat_m, p):\n",
    "      \"\"\"Computes mat_m^p, for p a positive integer.\n",
    "\n",
    "      Power p is known at graph compile time, so no need for loop and cond.\n",
    "      Args:\n",
    "        mat_m: a square matrix\n",
    "        p: a positive integer\n",
    "\n",
    "      Returns:\n",
    "        mat_m^p\n",
    "      \"\"\"\n",
    "      assert p == int(p) and p > 0\n",
    "      power = None\n",
    "      while p > 0:\n",
    "        if p % 2 == 1:\n",
    "          power = math_ops.matmul(mat_m, power) if power is not None else mat_m\n",
    "        p //= 2\n",
    "        mat_m = math_ops.matmul(mat_m, mat_m)\n",
    "      return power\n",
    "\n",
    "    def IterCondition(i, mat_m, _):\n",
    "      return math_ops.logical_and(\n",
    "          i < iter_count,\n",
    "          math_ops.reduce_max(math_ops.abs(mat_m - identity)) > epsilon)\n",
    "\n",
    "    def IterBody(i, mat_m, mat_x):\n",
    "      mat_m_i = (1 - alpha) * identity + alpha * mat_m\n",
    "      return (i + 1, math_ops.matmul(MatPower(mat_m_i, -1.0/alpha), mat_m),\n",
    "              math_ops.matmul(mat_x, mat_m_i))\n",
    "\n",
    "    if mat_g_size == 1:\n",
    "      mat_h = math_ops.pow(mat_g + self._epsilon, alpha)\n",
    "    else:\n",
    "      damped_mat_g = mat_g + self._epsilon * identity\n",
    "      z = (1 - 1 / alpha) / (2 * linalg_ops.norm(damped_mat_g))\n",
    "      # The best value for z is\n",
    "      # (1 - 1/alpha) * (c_max^{-alpha} - c_min^{-alpha}) /\n",
    "      #                 (c_max^{1-alpha} - c_min^{1-alpha})\n",
    "      # where c_max and c_min are the largest and smallest singular values of\n",
    "      # damped_mat_g.\n",
    "      # The above estimate assumes that c_max > c_min * 2^p. (p = -1/alpha)\n",
    "      # Can replace above line by the one below, but it is less accurate,\n",
    "      # hence needs more iterations to converge.\n",
    "      # z = (1 - 1/alpha) / math_ops.trace(damped_mat_g)\n",
    "      # If we want the method to always converge, use z = 1 / norm(damped_mat_g)\n",
    "      # or z = 1 / math_ops.trace(damped_mat_g), but these can result in many\n",
    "      # extra iterations.\n",
    "      _, _, mat_h = control_flow_ops.while_loop(\n",
    "          IterCondition, IterBody,\n",
    "          [0, damped_mat_g * z, identity * math_ops.pow(z, -alpha)])\n",
    "    if mat_h_slot_name is not None:\n",
    "      return state_ops.assign(self.get_slot(var, mat_h_slot_name), mat_h)\n",
    "    return mat_h\n",
    "\n",
    "  def _compute_power(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name=None):\n",
    "    \"\"\"Just a switch between the iterative power vs svd.\"\"\"\n",
    "    with ops.name_scope(\"matrix_iterative_power\"):\n",
    "      if self._use_iterative_root:\n",
    "        return self._compute_power_iter(var, mat_g, mat_g_size, alpha,\n",
    "                                        mat_h_slot_name)\n",
    "      else:\n",
    "        return self._compute_power_svd(var, mat_g, mat_g_size, alpha,\n",
    "                                       mat_h_slot_name)\n",
    "\n",
    "  def _apply_gradient(self, grad, var, indices=None):\n",
    "    \"\"\"The main function to update a variable.\n",
    "\n",
    "    Args:\n",
    "      grad: A Tensor containing gradient to apply.\n",
    "      var: A Tensor containing the variable to update.\n",
    "      indices: An array of integers, for sparse update.\n",
    "\n",
    "    Returns:\n",
    "      Updated variable var = var - learning_rate * preconditioner * grad\n",
    "\n",
    "    If the gradient is dense, var and grad have the same shape.\n",
    "    If the update is sparse, then the first dimension of the gradient and var\n",
    "    may differ, others are all the same. In this case the indices array\n",
    "    provides the set of indices of the variable which are to be updated with\n",
    "    each row of the gradient.\n",
    "    \"\"\"\n",
    "    global_step = self._global_step + 1\n",
    "\n",
    "    # Update accumulated weighted average of gradients\n",
    "    gbar = self.get_slot(var, \"gbar\")\n",
    "    gbar_decay_t = GetParam(self._gbar_decay, global_step)\n",
    "    gbar_weight_t = GetParam(self._gbar_weight, global_step)\n",
    "    if indices is not None:\n",
    "      # Note - the sparse update is not easily implemented, since the\n",
    "      # algorithm needs all indices of gbar to be updated\n",
    "      # if mat_gbar_decay != 1 or mat_gbar_decay != 0.\n",
    "      # One way to make mat_gbar_decay = 1 is by rescaling.\n",
    "      # If we want the update:\n",
    "      #         G_{t+1} = a_{t+1} G_t + b_{t+1} w_t\n",
    "      # define:\n",
    "      #         r_{t+1} = a_{t+1} * r_t\n",
    "      #         h_t = G_t / r_t\n",
    "      # Then:\n",
    "      #         h_{t+1} = h_t + (b_{t+1} / r_{t+1}) * w_t\n",
    "      # So we get the mat_gbar_decay = 1 as desired.\n",
    "      # We can implement this in a future version as needed.\n",
    "      # However we still need gbar_decay = 0, otherwise all indices\n",
    "      # of the variable will need to be updated.\n",
    "      if self._gbar_decay != 0.0:\n",
    "        tf_logging.warning(\"Not applying momentum for variable: %s\" % var.name)\n",
    "      gbar_updated = grad\n",
    "    else:\n",
    "      gbar_updated = self._weighted_average(gbar, self._gbar_decay,\n",
    "                                            gbar_decay_t,\n",
    "                                            gbar_weight_t * grad)\n",
    "\n",
    "    # Update the preconditioners and compute the preconditioned gradient\n",
    "    shape = var.get_shape()\n",
    "    mat_g_list = []\n",
    "    for i in range(len(shape)):\n",
    "      mat_g_list.append(self.get_slot(var, \"Gbar_\" + str(i)))\n",
    "    mat_gbar_decay_t = GetParam(self._mat_gbar_decay, global_step)\n",
    "    mat_gbar_weight_t = GetParam(self._mat_gbar_weight, global_step)\n",
    "\n",
    "    preconditioned_grad = gbar_updated\n",
    "    v_rank = len(mat_g_list)\n",
    "    neg_alpha = - GetParam(self._alpha, global_step) / v_rank\n",
    "    svd_interval = GetParam(self._svd_interval, global_step)\n",
    "    precond_update_interval = GetParam(self._precond_update_interval,\n",
    "                                       global_step)\n",
    "    for i, mat_g in enumerate(mat_g_list):\n",
    "      # axes is the list of indices to reduce - everything but the current i.\n",
    "      axes = list(range(i)) + list(range(i+1, v_rank))\n",
    "      if shape[i] <= self._max_matrix_size:\n",
    "        # If the tensor size is sufficiently small perform full Shampoo update\n",
    "        # Note if precond_update_interval > 1 and mat_gbar_decay_t != 1, this\n",
    "        # is not strictly correct. However we will use it for now, and\n",
    "        # fix if needed. (G_1 = aG + bg ==> G_n = a^n G + (1+a+..+a^{n-1})bg)\n",
    "\n",
    "        # pylint: disable=g-long-lambda,cell-var-from-loop\n",
    "        mat_g_updated = control_flow_ops.cond(\n",
    "            math_ops.mod(global_step, precond_update_interval) < 1,\n",
    "            lambda: self._update_mat_g(\n",
    "                mat_g, grad, axes, mat_gbar_decay_t,\n",
    "                mat_gbar_weight_t * precond_update_interval, i),\n",
    "            lambda: mat_g)\n",
    "\n",
    "        if self._svd_interval == 1:\n",
    "          mat_h = self._compute_power(var, mat_g_updated, shape[i], neg_alpha)\n",
    "        else:\n",
    "          mat_h = control_flow_ops.cond(\n",
    "              math_ops.mod(global_step, svd_interval) < 1,\n",
    "              lambda: self._compute_power(var, mat_g_updated, shape[i],\n",
    "                                          neg_alpha, \"H_\" + str(i)),\n",
    "              lambda: self.get_slot(var, \"H_\" + str(i)))\n",
    "\n",
    "        # mat_h is a square matrix of size d_i x d_i\n",
    "        # preconditioned_grad is a d_i x ... x d_n x d_0 x ... d_{i-1} tensor\n",
    "        # After contraction with a d_i x d_i tensor\n",
    "        # it becomes a d_{i+1} x ... x d_n x d_0 x ... d_i tensor\n",
    "        # (the first dimension is contracted out, and the second dimension of\n",
    "        # mat_h is appended).  After going through all the indices, it becomes\n",
    "        # a d_0 x ... x d_n tensor again.\n",
    "        preconditioned_grad = math_ops.tensordot(preconditioned_grad, mat_h,\n",
    "                                                 axes=([0], [0]),\n",
    "                                                 name=\"precond_\" + str(i))\n",
    "      else:\n",
    "        # Tensor size is too large -- perform diagonal Shampoo update\n",
    "        grad_outer = math_ops.reduce_sum(grad * grad, axis=axes)\n",
    "        if i == 0 and indices is not None:\n",
    "          assert self._mat_gbar_decay == 1.0\n",
    "          mat_g_updated = state_ops.scatter_add(mat_g, indices,\n",
    "                                                mat_gbar_weight_t * grad_outer)\n",
    "          mat_h = math_ops.pow(\n",
    "              array_ops.gather(mat_g_updated, indices) + self._epsilon,\n",
    "              neg_alpha)\n",
    "        else:\n",
    "          mat_g_updated = self._weighted_average(mat_g,\n",
    "                                                 self._mat_gbar_decay,\n",
    "                                                 mat_gbar_decay_t,\n",
    "                                                 mat_gbar_weight_t * grad_outer)\n",
    "          mat_h = math_ops.pow(mat_g_updated + self._epsilon, neg_alpha)\n",
    "\n",
    "        # Need to do the transpose to ensure that the tensor becomes\n",
    "        # a d_{i+1} x ... x d_n x d_0 x ... d_i tensor as described above.\n",
    "        preconditioned_grad = array_ops.transpose(\n",
    "            preconditioned_grad, perm=list(range(1, v_rank)) + [0]) * mat_h\n",
    "\n",
    "    # Update the variable based on the Shampoo update\n",
    "    learning_rate_t = GetParam(self._learning_rate, global_step)\n",
    "    if indices is not None:\n",
    "      var_updated = state_ops.scatter_add(\n",
    "          var, indices, -learning_rate_t * preconditioned_grad)\n",
    "    else:\n",
    "      var_updated = state_ops.assign_sub(var,\n",
    "                                         learning_rate_t * preconditioned_grad)\n",
    "    return var_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Client\n",
    "Based on mlagi@ implementation (added upload, fast file existence checking, some minor changes):\n",
    "\n",
    "Caching / logging is removed as this additional I/O slows things down by ~30-40%.\n",
    "\n",
    "https://git.hubteam.com/HubSpot/ProductionModels/blob/8d5196a59b256ff5a67caed3a7cce38cd39a393d/Searchbar/v1/prod/AWSGetter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "class S3Client(object):\n",
    "    \"\"\"Download/upload files from/to Amazon Web Services S3.\n",
    "    \n",
    "    If `aws_access_key_id` and `aws_secret_access_key` are not defined\n",
    "    it will attempt loading credentials from ~/.aws/credentials.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket_name, prefix, aws_access_key_id=None, aws_secret_access_key=None):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_subdir_path(subdir_path):\n",
    "        \"\"\"Removes '/' prefix if it exists.\n",
    "        \n",
    "        In case if subdirectory path is absolute (starts with '/') all\n",
    "        previous paths are discarded.\n",
    "        E.g.: os.path.join('/home', '/dir')  # Outputs: '/dir'\n",
    "        \"\"\"\n",
    "        return subdir_path[1:] if subdir_path.startswith('/') else subdir_path\n",
    "    \n",
    "    def check_file(self, key):\n",
    "        \"\"\"Check if file exists on S3.\"\"\"\n",
    "        aws_file_key = os.path.join(self.prefix, key)\n",
    "        try:\n",
    "            self.client.head_object(Bucket=self.bucket_name, Key=aws_file_key)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def get_file(self, key):\n",
    "        \"\"\"Read a file from S3 (without saving to disk).\"\"\"\n",
    "        aws_file_key = os.path.join(self.prefix, key)\n",
    "        try:\n",
    "            s3_object = self.client.get_object(Bucket=self.bucket_name, Key=aws_file_key)\n",
    "            return s3_object['Body']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    \n",
    "    def put_file(self, body, key):\n",
    "        \"\"\"Upload a file from S3 (without saving to disk).\"\"\"\n",
    "        aws_file_key = os.path.join(self.prefix, key)\n",
    "        try:\n",
    "            s3_object = self.client.put_object(\n",
    "                Body=body, Bucket=self.bucket_name, Key=aws_file_key)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def download_file(self, key, destination_dir, truncate_path=False):\n",
    "        \"\"\"Download a file from S3.\"\"\"\n",
    "        # Get file paths.\n",
    "        file_path = os.path.join(destination_dir,\n",
    "                                 os.path.split(key)[1] if truncate_path else key)\n",
    "        aws_file_key = os.path.join(self.prefix, key)\n",
    "        \n",
    "        # Create target subdirectories if needed.\n",
    "        folder_path = os.path.dirname(file_path)\n",
    "        if folder_path and (not os.path.exists(folder_path)):\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Download file.\n",
    "        self.client.download_file(self.bucket_name, aws_file_key, file_path)\n",
    "\n",
    "    def upload_file(self, file_path, destination_dir):\n",
    "        \"\"\"Upload a file to S3.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): path to a local file.\n",
    "            destination_dir (str): directory to which file will be uploaded.\n",
    "        \"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        key = os.path.join(\n",
    "            self.prefix, self.clean_subdir_path(destination_dir), file_name)\n",
    "        self.client.upload_file(\n",
    "            Filename=file_path, Bucket=self.bucket_name, Key=key)\n",
    "\n",
    "    def download_files(self, keys, destination_dir, truncate_path=False):\n",
    "        \"\"\"Download a list of files from S3.\"\"\"\n",
    "        for key in keys:\n",
    "            self.download_file(key, destination_dir, truncate_path)\n",
    "            \n",
    "    def upload_files(self, file_paths, destination_dir):\n",
    "        \"\"\"Upload a list of files to S3.\"\"\"\n",
    "        for file_path in file_paths:\n",
    "            self.upload_file(file_path, destination_dir)\n",
    "\n",
    "    def download_directory(self, prefix, destination_dir, original_prefix=None):\n",
    "        \"\"\"Download recursively all files from a S3 directory.\n",
    "        Args:\n",
    "            prefix (str): S3 directory path (e.g. \"qa/dumps/searchbar/\").\n",
    "            destination_dir (str): local destination directory.\n",
    "            original_prefix (str): first prefix in the recursion stack.\n",
    "        \"\"\"\n",
    "        paginator = self.client.get_paginator('list_objects')\n",
    "        if not prefix.startswith(self.prefix):\n",
    "            prefix = self.prefix + prefix\n",
    "        if original_prefix is None:\n",
    "            original_prefix = prefix\n",
    "        for result in paginator.paginate(Bucket=self.bucket_name, Delimiter='/',\n",
    "                                         Prefix=prefix):\n",
    "            if result.get('CommonPrefixes') is not None:\n",
    "                for subdir in result.get('CommonPrefixes'):\n",
    "                    self.download_directory(\n",
    "                        subdir.get('Prefix'), destination_dir,\n",
    "                        original_prefix=original_prefix)\n",
    "            if result.get('Contents') is not None:\n",
    "                for content in result.get('Contents'):\n",
    "                    key = content.get('Key')\n",
    "                    short_key = self.clean_subdir_path(\n",
    "                        key.replace(original_prefix, ''))\n",
    "                    short_path = os.path.join(destination_dir, short_key)\n",
    "                    dirname = os.path.dirname(short_path)\n",
    "                    if not os.path.exists(dirname):\n",
    "                        os.makedirs(dirname)\n",
    "                    if os.path.exists(short_path):\n",
    "                        continue\n",
    "                    try:\n",
    "                        self.client.download_file(\n",
    "                            self.bucket_name, key, short_path)\n",
    "                    except OSError as err:\n",
    "                        continue\n",
    "                        \n",
    "    def upload_directory(self, source_dir, destination_dir):\n",
    "        \"\"\"Upload a directory and all its contents to S3.\n",
    "        \n",
    "        Note: empty subfolders are not uploaded.\n",
    "        \"\"\"\n",
    "        for path, _, files in os.walk(source_dir):\n",
    "            for f in files:\n",
    "                dir_prefix = path.replace(source_dir, '')\n",
    "                file_destination_dir = self.clean_subdir_path(\n",
    "                    os.path.join(destination_dir, dir_prefix))\n",
    "                self.upload_file(os.path.join(path, f), file_destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}\n",
    "        \n",
    "\n",
    "def plot_clusters(clusters, xs, ys):\n",
    "    text_df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "    groups = text_df.groupby('label')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='o', linestyle='', ms=6, mec='none', alpha=0.3)\n",
    "        ax.set_aspect('auto')\n",
    "        ax.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "        ax.tick_params(axis='y', which='both', left='off', top='off', labelleft='off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_string_clusters_with_tooltip(strings, clusters, xs, ys):\n",
    "    text_df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, item_name=strings)).sample(10000)\n",
    "    groups = text_df.groupby('label')\n",
    "\n",
    "    css = \"\"\"\n",
    "    text.mpld3-text, div.mpld3-tooltip {\n",
    "      font-family:Arial, Helvetica, sans-serif;\n",
    "    }\n",
    "\n",
    "    g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "    display: none; }\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for name, group in groups:\n",
    "        if name == -1:\n",
    "            continue\n",
    "        \n",
    "        points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=6, mec='none', alpha=0.3)\n",
    "        ax.set_aspect('auto')\n",
    "        labels = [i for i in group.item_name]\n",
    "\n",
    "        tooltip = mpld3.plugins.PointHTMLTooltip(\n",
    "            points[0], labels, voffset=10, hoffset=10, css=css)\n",
    "        mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP_COMPLETE = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
