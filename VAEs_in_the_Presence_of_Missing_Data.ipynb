{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAEs in the Presence of Missing Data.ipynb",
      "provenance": [
        {
          "file_id": "18vIRf7MjwGYfA9tsNsEvXVL7pwXgwQ6S",
          "timestamp": 1585386680713
        },
        {
          "file_id": "115odpl67UU06i4FrTmVf-Cuyin-_s1xZ",
          "timestamp": 1574880615773
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl9_056FaTXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02CzoxsxaqRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "TEST_BATCH_SIZE = 1\n",
        "NUM_IMPORTANCE_SAMPLES = 256 # for test set marginal likelihood estimation\n",
        "MISSINGNESS_TYPE = 'MNAR' # MCAR or MNAR\n",
        "MISSINGNESS_COMPLEXITY = 'COMPLEX' # SIMPLE or COMPLEX\n",
        "MARGINAL_LL_MC_SAMPLES = 100\n",
        "DATASET = 'MNIST' # MNIST or SVHN\n",
        "LIKELIHOOD = 'BERNOULLI' # BERNOULLI or LOGISTIC_MIXTURE\n",
        "# DATASET = 'SVHN' # MNIST or SVHN\n",
        "# LIKELIHOOD = 'LOGISTIC_MIXTURE' # BERNOULLI or LOGISTIC_MIXTURE\n",
        "LOGISTIC_MIXTURE_COMPONENTS = 1\n",
        "if DATASET == 'MNIST':\n",
        "  Z_DIM = 50\n",
        "  img_dim = 28\n",
        "else:\n",
        "  Z_DIM = 200\n",
        "  img_dim = 32\n",
        "\n",
        "NUM_RUNS = 5\n",
        "VISUALIZE = False # plot reconstructions\n",
        "VERBOSE = False # print verbose updates after each training epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHbg2rw8aNmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_missingness(dataset, x_mean):\n",
        "  missingness_block_size = 7\n",
        "  if DATASET == 'MNIST':\n",
        "    num_missing_blocks = 8\n",
        "    mnar_blocks = [[5, 10], [5, 12], [7, 5], [7, 6], [9, 3], [9, 4], [11, 2],\n",
        "                   [11, 3], [13, 1], [15, 1]]\n",
        "  else:\n",
        "    num_missing_blocks = 9\n",
        "    mnar_blocks = [[5, 12], [7, 5], [7, 6], [9, 3], [9, 4], [11, 2], [11, 3],\n",
        "                   [13, 2], [15, 1], [17, 1]]\n",
        "  if MISSINGNESS_TYPE == 'MNAR':\n",
        "    mnar_blocks = np.random.permutation(mnar_blocks).tolist()\n",
        "\n",
        "  images = []\n",
        "  masks = []\n",
        "  zero_imp_images = []\n",
        "  mean_imp_images = []\n",
        "  labels = []\n",
        "  for example in dataset:\n",
        "    b = np.ones_like(example['image'], dtype=np.float32)\n",
        "    label = example['label']\n",
        "    if MISSINGNESS_TYPE == 'MCAR' and MISSINGNESS_COMPLEXITY == 'SIMPLE':\n",
        "      for _ in range(num_missing_blocks):\n",
        "        x = np.random.choice(img_dim)\n",
        "        y = np.random.choice(img_dim)\n",
        "        x_min = max(x - missingness_block_size//2, 0)\n",
        "        x_max = min(x + missingness_block_size//2, img_dim)\n",
        "        y_min = max(y - missingness_block_size//2, 0)\n",
        "        y_max = min(y + missingness_block_size//2, img_dim)\n",
        "        if DATASET == 'MNIST':\n",
        "          b[x_min:x_max, y_min:y_max] = 0.0\n",
        "        else:\n",
        "          b[x_min:x_max, y_min:y_max, :] = 0.0\n",
        "    elif MISSINGNESS_TYPE == 'MNAR' or MISSINGNESS_COMPLEXITY == 'COMPLEX':\n",
        "      if MISSINGNESS_TYPE == 'MNAR':\n",
        "        missingness_block_size, num_missing_blocks = mnar_blocks[int(label)]\n",
        "      else:\n",
        "        idx = np.random.choice(len(mnar_blocks))\n",
        "        missingness_block_size, num_missing_blocks = mnar_blocks[idx]\n",
        "      \n",
        "      for _ in range(num_missing_blocks):\n",
        "        x = missingness_block_size//2 + np.random.choice(\n",
        "            img_dim - missingness_block_size)\n",
        "        y = missingness_block_size//2 + np.random.choice(\n",
        "            img_dim - missingness_block_size)\n",
        "        x_min = max(x - missingness_block_size//2, 0)\n",
        "        x_max = min(x + missingness_block_size//2, img_dim)\n",
        "        y_min = max(y - missingness_block_size//2, 0)\n",
        "        y_max = min(y + missingness_block_size//2, img_dim)\n",
        "        if DATASET == 'MNIST':\n",
        "          b[x_min:x_max, y_min:y_max] = 0.0\n",
        "        else:\n",
        "          b[x_min:x_max, y_min:y_max, :] = 0.0\n",
        "    \n",
        "    images.append(example['image'])\n",
        "    masks.append(b)\n",
        "    zero_imp_images.append(b * example['image'])\n",
        "    mean_imp_images.append(b * example['image'] + (1.0 - b) * x_mean)\n",
        "    labels.append(label)\n",
        "  return tf.data.Dataset.from_tensor_slices(\n",
        "      {'x': images, 'x_zero_imp': zero_imp_images, 'b': masks,\n",
        "       'x_mean_imp': mean_imp_images, 'y': labels})\n",
        "\n",
        "def cast_img(example):\n",
        "  example['b'] = tf.cast(example['b'], tf.float32)\n",
        "  max_val = 255.0\n",
        "\n",
        "  example['x'] = tf.cast(example['x'], tf.float32) / max_val\n",
        "  example['x_zero_imp'] = tf.cast(example['x_zero_imp'], tf.float32) / max_val\n",
        "  example['x_mean_imp'] = tf.cast(example['x_mean_imp'], tf.float32) / max_val\n",
        "  return example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMN-kmBzZoiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_datasets():\n",
        "  if DATASET == 'MNIST':\n",
        "    train_ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
        "    test_ds = tfds.load('mnist', split='test', shuffle_files=False)\n",
        "  else:\n",
        "    train_ds = tfds.load('svhn_cropped', split='train', shuffle_files=True)\n",
        "    test_ds = tfds.load('svhn_cropped', split='test', shuffle_files=False)\n",
        "\n",
        "  train_ds = tfds.as_numpy(train_ds)\n",
        "  train_ds = [example for example in train_ds]\n",
        "  train_ds, valid_ds = train_test_split(\n",
        "      train_ds, train_size=50000 if DATASET == 'MNIST' else 63257,\n",
        "      test_size=10000)\n",
        "\n",
        "  test_ds = tfds.as_numpy(test_ds)\n",
        "  test_ds = [example for example in test_ds]\n",
        "\n",
        "  x_mean = np.mean(np.asarray([example['image'] for example in train_ds]), axis=0)\n",
        "\n",
        "  train_ds = generate_missingness(train_ds, x_mean)\n",
        "  valid_ds = generate_missingness(valid_ds, x_mean)\n",
        "  test_ds = generate_missingness(test_ds, x_mean)\n",
        "\n",
        "  train_ds = train_ds.map(cast_img).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  valid_ds = valid_ds.map(cast_img).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test_ds = test_ds.map(cast_img).batch(TEST_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return train_ds, valid_ds, test_ds\n",
        "\n",
        "train_ds, valid_ds, test_ds = generate_datasets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMBHQjUq939h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DS SIZES + CLASS COUNTS - SANITY CHECK\n",
        "\n",
        "for ds in [train_ds, valid_ds, test_ds]:\n",
        "  res = []\n",
        "  for example in ds:\n",
        "    res.append(tf.one_hot(example['y'], 10))\n",
        "  res = tf.concat(res, 0)\n",
        "  print(res.shape[0], tf.reduce_mean(res, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvtvRg1vph7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize data\n",
        "\n",
        "for example in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print('Label:', example['y'][i])\n",
        "    if DATASET == 'MNIST':\n",
        "      print('Original image')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(np.squeeze(example['x'][i]), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Missingness mask')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(np.squeeze(example['b'][i]), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Image with zero imputation')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(np.squeeze(example['x_zero_imp'][i]), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Image with mean imputation')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(np.squeeze(example['x_mean_imp'][i]), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "    else:\n",
        "      print('Original image')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(example['x'][i])\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Missingness mask')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(example['b'][i])\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Image with zero imputation')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(example['x_zero_imp'][i])\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      print('Image with mean imputation')\n",
        "      plt.figure(figsize=(1,1))\n",
        "      plt.imshow(example['x_mean_imp'][i])\n",
        "      plt.axis('off')\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i-QzBH9wUYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  \"\"\"VAE encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_shape):\n",
        "    \"\"\"Creates an instance of Encoder.\n",
        "\n",
        "    Returns:\n",
        "      Encoder instance.\n",
        "    \"\"\"\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    if DATASET == 'MNIST':\n",
        "      encoder_layers = [(32, 3, 2), (64, 3, 2)]\n",
        "    else:\n",
        "      encoder_layers = [(40, 3, 2), (60, 3, 2), (60, 5, 2)]\n",
        "\n",
        "    self.conv_layers = []\n",
        "    for i, (num_filters, kernel_size, strides) in enumerate(encoder_layers):\n",
        "      if i == 0:\n",
        "        self.conv_layers.append(tf.keras.layers.Conv2D(\n",
        "            num_filters, kernel_size, strides=strides, activation=tf.nn.relu,\n",
        "            data_format='channels_last', input_shape=input_shape,\n",
        "            padding='SAME'))\n",
        "      else:\n",
        "        self.conv_layers.append(tf.keras.layers.Conv2D(\n",
        "            num_filters, kernel_size, strides=strides, activation=tf.nn.relu,\n",
        "            data_format='channels_last', padding='SAME'))\n",
        "\n",
        "    self.mu_proj = tf.keras.layers.Dense(Z_DIM, activation=None)\n",
        "    self.sigma_proj = tf.keras.layers.Dense(Z_DIM, activation=tf.math.softplus)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"Computes the forward pass through the Encoder.\n",
        "\n",
        "    Args:\n",
        "      x: `Tensor`. 4-D `Tensor` of shape [batch_size, height, width, depth]\n",
        "        containing the input images.\n",
        "\n",
        "    Returns:\n",
        "      A tuple of `Tensors` of shape [batch_size, z_dim] the mean and sigma\n",
        "      parameters of a Gaussian distribution.\n",
        "    \"\"\"\n",
        "    for layer in self.conv_layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    return (self.mu_proj(x), (self.sigma_proj(x) + 1e-3))\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  \"\"\"VAE decoder.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Creates an instance of Decoder.\n",
        "\n",
        "    Returns:\n",
        "      Decoder instance.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    if DATASET == 'MNIST':\n",
        "      self.dense = tf.keras.layers.Dense(7*7*20, activation=tf.nn.relu)\n",
        "      self.reshape_shape = [-1, 7, 7, 20]\n",
        "      decoder_layers = [(40, 5, 2), (20, 5, 2)]\n",
        "      fine_tune_layers = [(10, 5, 1), (10, 5, 1)]\n",
        "      assert LIKELIHOOD == 'BERNOULLI'\n",
        "      last_layer = (1, 3, 1)\n",
        "    else:\n",
        "      self.dense = tf.keras.layers.Dense(4*4*60, activation=tf.nn.relu)\n",
        "      self.reshape_shape = [-1, 4, 4, 60]\n",
        "      decoder_layers = [(60, 3, 2), (60, 3, 2), (40, 5, 2)]\n",
        "      fine_tune_layers = [(30, 5, 1), (30, 5, 1)]\n",
        "      if LIKELIHOOD == 'LOGISTIC_MIXTURE':\n",
        "        last_layer = (9 * LOGISTIC_MIXTURE_COMPONENTS, 3, 1)\n",
        "      else:\n",
        "        last_layer = (3, 3, 1)\n",
        "\n",
        "    self.decoder_layers = []\n",
        "    for i, (num_filters, kernel_size, strides) in enumerate(decoder_layers):\n",
        "      self.decoder_layers.append(tf.keras.layers.Conv2DTranspose(\n",
        "          num_filters, kernel_size, strides=strides, activation=tf.nn.relu,\n",
        "          data_format='channels_last', padding='SAME'))\n",
        "    \n",
        "    self.fine_tune_layers = []\n",
        "    for i, (num_filters, kernel_size, strides) in enumerate(fine_tune_layers):\n",
        "      self.fine_tune_layers.append(tf.keras.layers.Conv2D(\n",
        "          num_filters, kernel_size, strides=strides, activation=tf.nn.relu,\n",
        "          data_format='channels_last', padding='SAME'))\n",
        "    \n",
        "    self.last_layer = tf.keras.layers.Conv2D(\n",
        "          last_layer[0], last_layer[1], strides=last_layer[2],\n",
        "          activation=None, data_format='channels_last', padding='SAME')\n",
        "\n",
        "  def call(self, x, b=None):\n",
        "    \"\"\"Computes the forward pass through the Decoder.\n",
        "\n",
        "    Args:\n",
        "      x: `Tensor`. 4-D `Tensor` of shape [batch_size, height, width, depth]\n",
        "        containing the input images.\n",
        "\n",
        "    Returns:\n",
        "      Tuple of three `Tensors` (mean_logit, scale_logit, pi_logit)\n",
        "    \"\"\"\n",
        "    x = self.dense(x)\n",
        "    x = tf.reshape(x, self.reshape_shape)\n",
        "    for layer in self.decoder_layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    if b is not None:\n",
        "      x = tf.concat([x, b], axis=-1)\n",
        "    \n",
        "    for layer in self.fine_tune_layers:\n",
        "      x = layer(x)\n",
        "    \n",
        "    x = self.last_layer(x)\n",
        "\n",
        "    if LIKELIHOOD == 'LOGISTIC_MIXTURE':\n",
        "      mean_logit = []\n",
        "      scale_logit = []\n",
        "      pi_logit = []\n",
        "      img_channels = 3\n",
        "      k = LOGISTIC_MIXTURE_COMPONENTS\n",
        "      for i in range(img_channels):\n",
        "        mean_logit.append(x[:, :, :, i*k:(i+1)*k])\n",
        "        scale_logit.append(x[:, :, :,\n",
        "                             (img_channels*k + i*k):(img_channels*k + (i+1)*k)])\n",
        "        pi_logit.append(x[:, :, :,\n",
        "                          (2*img_channels*k + i*k):(2*img_channels*k + (i+1)*k)])\n",
        "    else:\n",
        "        mean_logit = x\n",
        "        scale_logit = None\n",
        "        pi_logit = None\n",
        "    \n",
        "    return mean_logit, scale_logit, pi_logit\n",
        "\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "  def __init__(self, input_shape):\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = Encoder(input_shape)\n",
        "    self.decoder = Decoder()\n",
        "\n",
        "  def call(self, inputs, decoder_b=None):\n",
        "    mu, sigma = self.encoder(inputs)\n",
        "    q_z = tfp.distributions.Normal(mu, sigma)\n",
        "    \n",
        "    z_sample = q_z.sample()\n",
        "    return self.decoder(z_sample, b=decoder_b), q_z, z_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkRR-Sij7WCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_sum_exp(x):\n",
        "  \"\"\"credit: https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py\"\"\"\n",
        "  axis = len(x.get_shape())-1\n",
        "  m = tf.reduce_max(x, axis)\n",
        "  m2 = tf.reduce_max(x, axis, keepdims=True)\n",
        "  return m + tf.math.log(tf.reduce_sum(tf.exp(x-m2), axis))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDXi3Z0R832P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_probs(x, b, x_logits, scale_logit, pi_logit):\n",
        "  if LIKELIHOOD == 'BERNOULLI':\n",
        "    x_pred = tf.nn.sigmoid(x_logits)\n",
        "  elif LIKELIHOOD == 'LOGISTIC_MIXTURE':\n",
        "    img_channels = 3\n",
        "    scale = [tf.exp(scale_logit[i]) + 1e-2 for i in range(img_channels)]\n",
        "    pi = [tf.nn.softmax(pi_logit[i], axis=3) for i in range(img_channels)]\n",
        "\n",
        "    if LOGISTIC_MIXTURE_COMPONENTS == 1:\n",
        "      x_pred = tf.concat([tf.reduce_sum(x_logits[i], axis=3, keepdims=True)\n",
        "                          for i in range(img_channels)], axis=3)\n",
        "    else:\n",
        "      x_pred = tf.concat(\n",
        "          [tf.reduce_sum(pi[i] * x_logits[i], axis=3, keepdims=True)\n",
        "           for i in range(img_channels)], axis=3)\n",
        "\n",
        "  if LIKELIHOOD == 'BERNOULLI':\n",
        "    # valid for even real valued MNIST: http://ruishu.io/2018/03/19/bernoulli-vae/\n",
        "    log_prob_full = -1 * tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=x, logits=x_logits)\n",
        "\n",
        "    log_prob = tf.reduce_sum(b * log_prob_full, axis=[3, 2,1])\n",
        "    imputation_log_prob = tf.reduce_sum((1.0 - b) * log_prob_full,\n",
        "                                        axis=[3, 2,1])\n",
        "  elif LIKELIHOOD == 'LOGISTIC_MIXTURE':\n",
        "    log_prob_full = []\n",
        "    for i in range(img_channels):\n",
        "      x_expanded = tf.concat([tf.expand_dims(x[:, :, :, i], axis=3)\n",
        "                              for _ in range(LOGISTIC_MIXTURE_COMPONENTS)], axis=3)\n",
        "      centered_x = x_expanded - x_logits[i]\n",
        "      upper_in = (centered_x + (1.0/255.0))/scale[i]\n",
        "      upper_cdf = tf.nn.sigmoid(upper_in)\n",
        "      lower_in = (centered_x - (1.0/255.0))/scale[i]\n",
        "      lower_cdf = tf.nn.sigmoid(lower_in)\n",
        "      cdf_delta = upper_cdf - lower_cdf\n",
        "      \n",
        "      mid_in = centered_x/scale[i]\n",
        "      log_pdf_mid = mid_in - scale_logit[i] - 2.0 * tf.nn.softplus(mid_in)\n",
        "      log_cdf_plus = upper_in - tf.nn.softplus(upper_in)\n",
        "      log_one_minus_cdf_min = -tf.nn.softplus(lower_in)\n",
        "      \n",
        "      log_prob_comp = tf.where(\n",
        "          x_expanded < -0.999,\n",
        "          log_cdf_plus,\n",
        "          tf.where(x_expanded > 0.999,\n",
        "                   log_one_minus_cdf_min,\n",
        "                   tf.where(cdf_delta > 1e-5,\n",
        "                            tf.math.log(tf.maximum(cdf_delta, 1e-12)),\n",
        "                            log_pdf_mid - np.log(127.5))))\n",
        "      if LOGISTIC_MIXTURE_COMPONENTS > 1:\n",
        "        log_prob_comp += tf.math.log(pi[i])\n",
        "\n",
        "      log_prob_full.append(log_sum_exp(log_prob_comp))\n",
        "\n",
        "    log_prob_full = tf.concat([tf.expand_dims(log_prob_full[i], axis=3)\n",
        "                                for i in range(img_channels)], axis=3)\n",
        "\n",
        "    log_prob = tf.reduce_sum(b * log_prob_full, axis=[3,2,1])\n",
        "    imputation_log_prob = tf.reduce_sum((1.0 - b) * log_prob_full, axis=[3,2,1])\n",
        "\n",
        "  return x_pred, log_prob, imputation_log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uDmSNRIe0Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_kl(q_z):\n",
        "  p_z = tfp.distributions.Normal(loc=np.zeros(Z_DIM, dtype=np.float32),\n",
        "                                 scale=np.ones(Z_DIM, dtype=np.float32))\n",
        "  return tf.reduce_mean(\n",
        "      tf.reduce_sum(q_z.kl_divergence(p_z), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86RB_cDTfWQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(q_z, x, b, x_logits, scale_logit, pi_logit):\n",
        "  x_pred, log_prob, imputation_log_prob = log_probs(\n",
        "      x, b, x_logits, scale_logit, pi_logit)\n",
        "  log_prob = tf.reduce_mean(log_prob)\n",
        "  imputation_log_prob = tf.reduce_mean(imputation_log_prob)\n",
        "  kl = compute_kl(q_z)\n",
        "  return (-log_prob + kl, x_pred, log_prob, imputation_log_prob, kl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2cwZ8oUXqHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mse(b, x, x_pred):\n",
        "  sqe = (x - x_pred)**2\n",
        "  mse_xo = tf.reduce_sum(b * sqe)/tf.reduce_sum(b)\n",
        "  mse_xm = tf.reduce_sum((1.0 - b) * sqe)/tf.reduce_sum(1.0 - b)\n",
        "  return mse_xo, mse_xm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPe-zZEzdAcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_marginal_likelihood_estimate(z_sample, q_z, x, b, x_logits,\n",
        "                                         scale_logit, pi_logit):\n",
        "  # importance sampled estimate of the marginal likelihood\n",
        "  _, log_p_x_given_z, _ = log_probs(x, b, x_logits, scale_logit, pi_logit)\n",
        "\n",
        "  p_z = tfp.distributions.Normal(loc=np.zeros(Z_DIM, dtype=np.float32),\n",
        "                                 scale=np.ones(Z_DIM, dtype=np.float32))\n",
        "  log_p_z = tf.reduce_sum(p_z.log_prob(z_sample), -1)\n",
        "  log_q_z_given_x = tf.reduce_sum(q_z.log_prob(z_sample), -1)\n",
        "\n",
        "  log_s = tf.math.log(tf.constant(NUM_IMPORTANCE_SAMPLES, tf.float32))\n",
        "\n",
        "  marginal_ll = log_sum_exp(log_p_x_given_z + log_p_z - log_q_z_given_x) - log_s\n",
        "\n",
        "  ln_2 = tf.math.log(tf.constant(2.0, tf.float32))\n",
        "  num_obs_pixels = tf.reduce_sum(b[0, :, :, 0])\n",
        "  bits_per_pixel = -(marginal_ll / ln_2) / num_obs_pixels\n",
        "\n",
        "  return marginal_ll, bits_per_pixel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9vWNIQ47wCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METHODS = ['Zero Imputation', 'Zero Imputation Encoder Indicators',\n",
        "           'Zero Imputation Encoder Decoder Indicators']\n",
        "metrics = {METHOD: {'elbo': [], 'kl': [], 'log_prob': [],\n",
        "                    'imputation_log_prob': [], 'mse_xo': [], 'mse_xm': [],\n",
        "                    'marginal_ll': [], 'bits_per_pixel': [], 'accuracy': [],\n",
        "                    'knn_accuracy': []}\n",
        "           for METHOD in METHODS}\n",
        "\n",
        "for run in range(NUM_RUNS):\n",
        "  train_ds, valid_ds, test_ds = generate_datasets()\n",
        "  for METHOD in METHODS:\n",
        "    tf.random.set_seed(run)\n",
        "    np.random.seed(run)\n",
        "    random.seed(run)\n",
        "\n",
        "    print(run, METHOD)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    train_kl_metric = tf.keras.metrics.Mean(name='train_kl')\n",
        "    train_log_prob_metric = tf.keras.metrics.Mean(name='train_log_prob')\n",
        "    train_imputation_log_prob_metric = tf.keras.metrics.Mean(\n",
        "        name='train_imputation_log_prob')\n",
        "    train_elbo_metric = tf.keras.metrics.Mean(name='train_elbo')\n",
        "    train_mse_xo_metric = tf.keras.metrics.Mean(name='train_mse_xo')\n",
        "    train_mse_xm_metric = tf.keras.metrics.Mean(name='train_mse_xm')\n",
        "\n",
        "    valid_kl_metric = tf.keras.metrics.Mean(name='valid_kl')\n",
        "    valid_log_prob_metric = tf.keras.metrics.Mean(name='valid_log_prob')\n",
        "    valid_imputation_log_prob_metric = tf.keras.metrics.Mean(\n",
        "        name='valid_imputation_log_prob')\n",
        "    valid_elbo_metric = tf.keras.metrics.Mean(name='valid_elbo')\n",
        "    valid_mse_xo_metric = tf.keras.metrics.Mean(name='valid_mse_xo')\n",
        "    valid_mse_xm_metric = tf.keras.metrics.Mean(name='valid_mse_xm')\n",
        "\n",
        "    test_kl_metric = tf.keras.metrics.Mean(name='test_kl')\n",
        "    test_log_prob_metric = tf.keras.metrics.Mean(name='test_log_prob')\n",
        "    test_imputation_log_prob_metric = tf.keras.metrics.Mean(\n",
        "        name='test_imputation_log_prob')\n",
        "    test_elbo_metric = tf.keras.metrics.Mean(name='test_elbo')\n",
        "    test_mse_xo_metric = tf.keras.metrics.Mean(name='test_mse_xo')\n",
        "    test_mse_xm_metric = tf.keras.metrics.Mean(name='test_mse_xm')\n",
        "    test_marginal_ll_metric = tf.keras.metrics.Mean(name='test_marginal_ll')\n",
        "    test_bits_per_pixel_metric = tf.keras.metrics.Mean(\n",
        "        name='test_bits_per_pixel')\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, b, inputs, decoder_b, model):\n",
        "      \"\"\"Defines a single training step: Update weights based on one batch.\"\"\"\n",
        "      with tf.GradientTape() as tape:\n",
        "        (x_logits, scale_logit, pi_logit), q_z, _ = model(inputs, decoder_b)\n",
        "        loss_value, x_pred, log_prob, imputation_log_prob, kl = loss_fn(\n",
        "            q_z, x, b, x_logits, scale_logit, pi_logit)\n",
        "\n",
        "      grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "      grads, _ = tf.clip_by_global_norm(grads, 2.5)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "      mse_xo, mse_xm = compute_mse(b, x, x_pred)\n",
        "\n",
        "      train_kl_metric(kl)\n",
        "      train_log_prob_metric(log_prob)\n",
        "      train_imputation_log_prob_metric(imputation_log_prob)\n",
        "      train_elbo_metric(-loss_value)\n",
        "      train_mse_xo_metric(mse_xo)\n",
        "      train_mse_xm_metric(mse_xm)\n",
        "\n",
        "    @tf.function\n",
        "    def eval_step(x, b, inputs, decoder_b, model, validation_set=False):\n",
        "      \"\"\"Get model predictions for one batch and update metrics.\"\"\"\n",
        "      (x_logits, scale_logit, pi_logit), q_z, z_sample = model(\n",
        "          inputs, decoder_b)\n",
        "      loss_value, x_pred, log_prob, imputation_log_prob, kl = loss_fn(\n",
        "          q_z, x, b, x_logits, scale_logit, pi_logit)\n",
        "      \n",
        "      mse_xo, mse_xm = compute_mse(b, x, x_pred)\n",
        "\n",
        "      if validation_set:\n",
        "        valid_kl_metric(kl)\n",
        "        valid_log_prob_metric(log_prob)\n",
        "        valid_imputation_log_prob_metric(imputation_log_prob)\n",
        "        valid_elbo_metric(-loss_value)\n",
        "        valid_mse_xo_metric(mse_xo)\n",
        "        valid_mse_xm_metric(mse_xm)\n",
        "      else:\n",
        "        test_kl_metric(kl)\n",
        "        test_log_prob_metric(log_prob)\n",
        "        test_imputation_log_prob_metric(imputation_log_prob)\n",
        "        test_elbo_metric(-loss_value)\n",
        "        test_mse_xo_metric(mse_xo)\n",
        "        test_mse_xm_metric(mse_xm)\n",
        "\n",
        "        marginal_ll, bits_per_pixel = compute_marginal_likelihood_estimate(\n",
        "            z_sample, q_z, x, b, x_logits, scale_logit, pi_logit)\n",
        "        test_marginal_ll_metric(marginal_ll)\n",
        "        test_bits_per_pixel_metric(bits_per_pixel)\n",
        "\n",
        "    if DATASET == 'MNIST':\n",
        "      if 'Indicators' in METHOD:\n",
        "        input_shape = [28, 28, 2]\n",
        "      else:\n",
        "        input_shape = [28, 28, 1]\n",
        "    else:\n",
        "      if 'Indicators' in METHOD:\n",
        "        input_shape = [32, 32, 4]\n",
        "      else:\n",
        "        input_shape = [32, 32, 3]\n",
        "\n",
        "    model = VAE(input_shape)\n",
        "\n",
        "    def get_inputs(example):\n",
        "      if METHOD == 'Zero Imputation':\n",
        "        return example['x_zero_imp'], None\n",
        "      elif METHOD == 'Mean Imputation':\n",
        "        return example['x_mean_imp'], None\n",
        "      elif METHOD == 'Zero Imputation Encoder Indicators':\n",
        "        b = tf.expand_dims(example['b'][:, :, :, 0], -1)\n",
        "        return tf.concat([example['x_zero_imp'], b], -1), None\n",
        "      elif METHOD == 'Zero Imputation Encoder Decoder Indicators':\n",
        "        b = tf.expand_dims(example['b'][:, :, :, 0], -1)\n",
        "        return tf.concat([example['x_zero_imp'], b], -1), b\n",
        "        \n",
        "    best_valid_elbo = None\n",
        "    num_epochs_since_improvement = 0\n",
        "    saved_model_loc = 'best_model.h5'\n",
        "    if os.path.exists(saved_model_loc):\n",
        "      os.remove(saved_model_loc)\n",
        "\n",
        "    for epoch in range(200):\n",
        "      for example in train_ds:\n",
        "        inputs, decoder_b = get_inputs(example)\n",
        "        train_step(example['x'], example['b'], inputs, decoder_b, model)\n",
        "\n",
        "      kl = train_kl_metric.result().numpy()\n",
        "      log_prob = train_log_prob_metric.result().numpy()\n",
        "      imputation_log_prob = train_imputation_log_prob_metric.result().numpy()\n",
        "      elbo = train_elbo_metric.result().numpy()\n",
        "      mse_xo = train_mse_xo_metric.result().numpy()\n",
        "      mse_xm = train_mse_xm_metric.result().numpy()\n",
        "\n",
        "      if VERBOSE:\n",
        "        print(f'Train set evaluation epoch: {epoch} - '\n",
        "              f'ELBO: {elbo}, KL: {kl}, log prob: {log_prob}, '\n",
        "              f'imp log prob: {imputation_log_prob}, '\n",
        "              f'mse_xo {mse_xo}, mse_xm: {mse_xm}')\n",
        "\n",
        "      train_kl_metric.reset_states()\n",
        "      train_log_prob_metric.reset_states()\n",
        "      train_imputation_log_prob_metric.reset_states()\n",
        "      train_elbo_metric.reset_states()\n",
        "      train_mse_xo_metric.reset_states()\n",
        "      train_mse_xm_metric.reset_states()\n",
        "\n",
        "      for example in valid_ds:\n",
        "        inputs, decoder_b = get_inputs(example)\n",
        "        eval_step(example['x'], example['b'], inputs, decoder_b, model,\n",
        "                  validation_set=True)\n",
        "\n",
        "      kl = valid_kl_metric.result().numpy()\n",
        "      log_prob = valid_log_prob_metric.result().numpy()\n",
        "      imputation_log_prob = valid_imputation_log_prob_metric.result().numpy()\n",
        "      elbo = valid_elbo_metric.result().numpy()\n",
        "      mse_xo = valid_mse_xo_metric.result().numpy()\n",
        "      mse_xm = valid_mse_xm_metric.result().numpy()\n",
        "\n",
        "      if VERBOSE:\n",
        "        print(f'Valid set evaluation epoch: {epoch} - '\n",
        "              f'ELBO: {elbo}, KL: {kl}, log prob: {log_prob}, '\n",
        "              f'imp log prob: {imputation_log_prob}, '\n",
        "              f'mse_xo {mse_xo}, mse_xm: {mse_xm}')\n",
        "\n",
        "      valid_kl_metric.reset_states()\n",
        "      valid_log_prob_metric.reset_states()\n",
        "      valid_imputation_log_prob_metric.reset_states()\n",
        "      valid_elbo_metric.reset_states()\n",
        "      valid_mse_xo_metric.reset_states()\n",
        "      valid_mse_xm_metric.reset_states()\n",
        "\n",
        "      if best_valid_elbo is None or elbo > best_valid_elbo:\n",
        "        best_valid_elbo = elbo\n",
        "        num_epochs_since_improvement = 0\n",
        "        model.save_weights(saved_model_loc)\n",
        "      elif num_epochs_since_improvement >= 10:\n",
        "        break\n",
        "      else:\n",
        "        num_epochs_since_improvement += 1\n",
        "\n",
        "    model.load_weights(saved_model_loc)\n",
        "    for example in test_ds:\n",
        "      for k in ['x', 'x_zero_imp', 'b', 'x_mean_imp', 'y']:\n",
        "        example[k] = tf.repeat(example[k], NUM_IMPORTANCE_SAMPLES, axis=0)\n",
        "      inputs, decoder_b = get_inputs(example)\n",
        "      eval_step(example['x'], example['b'], inputs, decoder_b, model,\n",
        "                validation_set=False)\n",
        "\n",
        "    kl = test_kl_metric.result().numpy()\n",
        "    log_prob = test_log_prob_metric.result().numpy()\n",
        "    imputation_log_prob = test_imputation_log_prob_metric.result().numpy()\n",
        "    elbo = test_elbo_metric.result().numpy()\n",
        "    mse_xo = test_mse_xo_metric.result().numpy()\n",
        "    mse_xm = test_mse_xm_metric.result().numpy()\n",
        "    marginal_ll = test_marginal_ll_metric.result().numpy()\n",
        "    bits_per_pixel = test_bits_per_pixel_metric.result().numpy()\n",
        "\n",
        "    # generate datasets for representation learning\n",
        "    def gen_rep_learning_datasets(ds):\n",
        "      z, y = [], []\n",
        "      for example in ds:\n",
        "        inputs, decoder_b = get_inputs(example)\n",
        "        z_mu, _ = model.encoder(inputs)\n",
        "        z.append(z_mu)\n",
        "        y.append(example['y'])\n",
        "      z = tf.concat(z, axis=0).numpy()\n",
        "      y = tf.concat(y, axis=0).numpy()\n",
        "      return z, y\n",
        "\n",
        "    z_train, y_train = gen_rep_learning_datasets(train_ds)\n",
        "    z_test, y_test = gen_rep_learning_datasets(test_ds)\n",
        "\n",
        "    # fit logistic classifier\n",
        "    classifier = LogisticRegression(\n",
        "        penalty='none', multi_class='multinomial', solver='lbfgs',\n",
        "        max_iter=1000)\n",
        "    classifier.fit(z_train, y_train)\n",
        "    accuracy = classifier.score(z_test, y_test)\n",
        "\n",
        "    # fit KNN classifier\n",
        "    classifier = KNeighborsClassifier()\n",
        "    classifier.fit(z_train, y_train)\n",
        "    knn_accuracy = classifier.score(z_test, y_test)\n",
        "\n",
        "    print(f'Test set evaluation epoch: {epoch} - '\n",
        "          f'bits per pixel: {bits_per_pixel}, marginal ll: {marginal_ll}, '\n",
        "          f'ELBO: {elbo}, KL: {kl}, log prob: {log_prob}, '\n",
        "          f'imp log prob: {imputation_log_prob}, '\n",
        "          f'mse_xo {mse_xo}, mse_xm: {mse_xm}, '\n",
        "          f'accuracy: {accuracy}, knn accuracy: {knn_accuracy}')\n",
        "    \n",
        "    metrics[METHOD]['kl'].append(kl)\n",
        "    metrics[METHOD]['log_prob'].append(log_prob)\n",
        "    metrics[METHOD]['imputation_log_prob'].append(imputation_log_prob)\n",
        "    metrics[METHOD]['elbo'].append(elbo)\n",
        "    metrics[METHOD]['mse_xo'].append(mse_xo)\n",
        "    metrics[METHOD]['mse_xm'].append(mse_xm)\n",
        "    metrics[METHOD]['marginal_ll'].append(marginal_ll)\n",
        "    metrics[METHOD]['bits_per_pixel'].append(bits_per_pixel)\n",
        "    metrics[METHOD]['accuracy'].append(accuracy)\n",
        "    metrics[METHOD]['knn_accuracy'].append(knn_accuracy)\n",
        "\n",
        "    # visualize data\n",
        "    if VISUALIZE:\n",
        "      for example in valid_ds.take(1):\n",
        "        inputs, decoder_b = get_inputs(example)\n",
        "        (x_logits, scale_logit, pi_logit), q_z, z_sample = model(\n",
        "            inputs, decoder_b)\n",
        "        _, x_pred, _, _, kl = loss_fn(\n",
        "            q_z, example['x'], example['b'], x_logits, scale_logit, pi_logit)\n",
        "\n",
        "        for i in range(25):\n",
        "          print('Label:', example['y'][i])\n",
        "          if DATASET == 'MNIST':\n",
        "            print('Original image')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(np.squeeze(example['x'][i]), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Missingness mask')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(np.squeeze(example['b'][i]), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Image with zero imputation')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(np.squeeze(example['x_zero_imp'][i]), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Image with mean imputation')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(np.squeeze(example['x_mean_imp'][i]), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Mean reconstruction')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(np.squeeze(x_pred[i]), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "          else:\n",
        "            print('Original image')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(example['x'][i])\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Missingness mask')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(example['b'][i])\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Image with zero imputation')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(example['x_zero_imp'][i])\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Image with mean imputation')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(example['x_mean_imp'][i])\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            print('Mean reconstruction')\n",
        "            plt.figure(figsize=(1,1))\n",
        "            plt.imshow(x_pred[i])\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "for METHOD in METHODS:\n",
        "  print('-' * 30)\n",
        "  print(METHOD)\n",
        "  print('-' * 30)\n",
        "  kl = np.mean(metrics[METHOD]['kl'])\n",
        "  log_prob = np.mean(metrics[METHOD]['log_prob'])\n",
        "  imputation_log_prob = np.mean(metrics[METHOD]['imputation_log_prob'])\n",
        "  elbo = np.mean(metrics[METHOD]['elbo'])\n",
        "  mse_xo = np.mean(metrics[METHOD]['mse_xo'])\n",
        "  mse_xm = np.mean(metrics[METHOD]['mse_xm'])\n",
        "  marginal_ll = np.mean(metrics[METHOD]['marginal_ll'])\n",
        "  bits_per_pixel = np.mean(metrics[METHOD]['bits_per_pixel'])\n",
        "  accuracy = np.mean(metrics[METHOD]['accuracy'])\n",
        "  knn_accuracy = np.mean(metrics[METHOD]['knn_accuracy'])\n",
        "  print(f'Average test set metrics - '\n",
        "        f'bits per pixel: {bits_per_pixel}, marginal_ll: {marginal_ll}, '\n",
        "        f'ELBO: {elbo}, KL: {kl}, log prob: {log_prob}, '\n",
        "        f'imp log prob: {imputation_log_prob}, '\n",
        "        f'mse_xo {mse_xo}, mse_xm: {mse_xm}, '\n",
        "        f'accuracy: {accuracy}, knn accuracy: {knn_accuracy}')\n",
        "  \n",
        "  for metric in ['imputation_log_prob', 'mse_xo', 'mse_xm', 'marginal_ll',\n",
        "                 'bits_per_pixel', 'accuracy', 'knn_accuracy']:\n",
        "    print(metric)\n",
        "    print(','.join(list(map(str, metrics[METHOD][metric]))))\n",
        "\n",
        "for metric in ['imputation_log_prob', 'elbo', 'mse_xo', 'mse_xm', 'marginal_ll',\n",
        "               'bits_per_pixel', 'accuracy', 'knn_accuracy']:\n",
        "  print('-' * 30)\n",
        "  print(metric)\n",
        "  print('-' * 30)\n",
        "  for METHOD_1 in METHODS:\n",
        "    for METHOD_2 in METHODS:\n",
        "      if METHOD_1 != METHOD_2:\n",
        "        p_value = stats.ttest_rel(\n",
        "            metrics[METHOD_1][metric], metrics[METHOD_2][metric], axis=0)[1]\n",
        "        print(f'{METHOD_1} vs. {METHOD_2}: {p_value}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}