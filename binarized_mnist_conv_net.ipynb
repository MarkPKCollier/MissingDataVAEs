{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Missing Data Imputation\n",
    "\n",
    "We shall compare both autoencoders and VAEs for the task of missing data imputation on the MNIST dataset.\n",
    "\n",
    "The variants of VAEs we shall compare are 1) vanilla VAE, 2) GMVAE, 3) concrete GMVAE.\n",
    "\n",
    "As inputs to the encoder in each case we compare 1) setting missing points to zero, 2) setting missing points to their mean on the training set, 3) setting missing points to zero and associating a indicator variable for missingness.\n",
    "\n",
    "We also compare the impact of an additional reconstruction term on the missingness indicators for both autoencoders and VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train > 0.5\n",
    "x_test = x_test > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=50000, test_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_dim = 128\n",
    "# input_units = x_train.shape[1] * x_train.shape[2]\n",
    "# encoder_layers = [(32, 5, 1), (64, 5, 1), 256]\n",
    "# decoder_layers = [256, 7*7*64,\n",
    "#                   ([-1, 7, 7, 64], [5, 5, 32, 64], 32, [1, 2, 2, 1], [-1, 14, 14, 32], tf.nn.relu),\n",
    "#                   (None, [5, 5, 2, 32], 2, [1, 2, 2, 1], [-1, 28, 28, None], None)]\n",
    "# max_epochs = 75\n",
    "# max_epoch_without_improvement = 10\n",
    "# mnist_dim = x_train.shape[1]\n",
    "\n",
    "z_dim = 128\n",
    "input_units = x_train.shape[1] * x_train.shape[2]\n",
    "encoder_layers = [(8, 5, 1), (16, 5, 1), 256]\n",
    "decoder_layers = [256, 7*7*16,\n",
    "                  ([-1, 7, 7, 16], [5, 5, 8, 16], 8, [1, 2, 2, 1], [-1, 14, 14, 8], tf.nn.relu),\n",
    "                  (None, [5, 5, None, 8], 2, [1, 2, 2, 1], [-1, 28, 28, None], None)]\n",
    "max_epochs = 60\n",
    "max_epoch_without_improvement = 5\n",
    "mnist_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "    net = inputs\n",
    "    for layer in encoder_layers:\n",
    "        if isinstance(layer, tuple):\n",
    "            filters, kernel_size, strides = layer\n",
    "            net = tf.layers.conv2d(net, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                   padding='SAME', activation=tf.nn.relu)\n",
    "            net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        else:\n",
    "            net = tf.contrib.layers.fully_connected(net, layer, activation_fn=tf.nn.relu)\n",
    "    \n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    mu = tf.contrib.layers.fully_connected(net, z_dim, activation_fn=None)\n",
    "    sigma = tf.contrib.layers.fully_connected(net, z_dim, activation_fn=tf.nn.softplus)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "def decoder(z, recon_b=False):\n",
    "    net = z\n",
    "    for i, layer in enumerate(decoder_layers):\n",
    "        if isinstance(layer, tuple):\n",
    "            input_size, weights, bias, strides, output_shape, activation = layer\n",
    "            if input_size is not None:\n",
    "                net = tf.reshape(net, input_size)\n",
    "            output_shape = [tf.shape(net)[0]] + output_shape[1:]\n",
    "            if output_shape[-1] == None:\n",
    "                output_shape[-1] = 2 if recon_b else 1\n",
    "                weights[-2] = 2 if recon_b else 1\n",
    "            \n",
    "            w = tf.get_variable('decoder_weights_{0}'.format(i), weights,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable('decoder_bias_{0}'.format(i), bias,\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            net = tf.nn.conv2d_transpose(net, w, output_shape=output_shape, strides=strides)\n",
    "            net = net + b\n",
    "            if activation is not None:\n",
    "                net = activation(net)\n",
    "        else:\n",
    "            net = tf.contrib.layers.fully_connected(net, layer, activation_fn=tf.nn.relu)\n",
    "    \n",
    "    p_net = tf.contrib.layers.flatten(net[:, :, :, 0])\n",
    "    p = tf.contrib.layers.fully_connected(p_net, mnist_dim * mnist_dim, activation_fn=None)\n",
    "    if recon_b:\n",
    "        b_net = tf.contrib.layers.flatten(net[:, :, :, 1])\n",
    "        b = tf.contrib.layers.fully_connected(b_net, mnist_dim * mnist_dim, activation_fn=None)\n",
    "        return p, b\n",
    "    \n",
    "    return p, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode, params):\n",
    "    x = tf.reshape(tf.feature_column.input_layer(features, params['feature_columns'][0]),\n",
    "                   [-1, mnist_dim, mnist_dim])\n",
    "    b = tf.reshape(tf.feature_column.input_layer(features, params['feature_columns'][1]),\n",
    "                   [-1, mnist_dim, mnist_dim])\n",
    "    recon_b = 'recon_b' in params['model_type']\n",
    "    \n",
    "    mu, sigma = encoder(tf.stack([x, b], axis=3) if '_ind' in params['model_type'] else tf.expand_dims(x, -1))\n",
    "    \n",
    "    q_z = tf.distributions.Normal(mu, sigma)\n",
    "    \n",
    "    p_z = tf.distributions.Normal(loc=np.zeros(z_dim, dtype=np.float32), scale=np.ones(z_dim, dtype=np.float32))\n",
    "        \n",
    "    decoder_inputs = z_sample = q_z.sample()\n",
    "    if 'dec_cond_b' in params['model_type']:\n",
    "        decoder_inputs = tf.concat([z_sample, tf.contrib.layers.flatten(b)], axis=1)\n",
    "    x_logits, b_logits = decoder(decoder_inputs, recon_b=recon_b)\n",
    "    x_logits = tf.reshape(x_logits, [-1, mnist_dim, mnist_dim])\n",
    "    if b_logits is not None:\n",
    "        b_logits = tf.reshape(b_logits, [-1, mnist_dim, mnist_dim])\n",
    "    x_pred = tf.nn.sigmoid(x_logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode,\n",
    "            predictions={'x': x_pred, 'z': mu},\n",
    "            export_outputs={'y': tf.estimator.export.ClassificationOutput(scores=x_pred)})\n",
    "    \n",
    "#     p_x_z = tf.distributions.Bernoulli(probs=x_mu)\n",
    "    \n",
    "    kl = tf.reduce_mean(tf.reduce_sum(tf.distributions.kl_divergence(q_z, p_z), axis=1))\n",
    "    log_prob = -1 * tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(\n",
    "        b * tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_logits), axis=2), axis=1))\n",
    "#     log_prob = tf.reduce_mean(tf.reduce_sum(p_x_z.log_prob(inputs), axis=1))\n",
    "    if recon_b:\n",
    "        log_prob_b = -1 * tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=b, logits=b_logits), axis=2), axis=1))\n",
    "        \n",
    "        loss = -1 * log_prob - log_prob_b + kl\n",
    "    else:\n",
    "        loss = -1 * log_prob + kl\n",
    "\n",
    "    tf.summary.scalar('kl', kl)\n",
    "    tf.summary.scalar('log_prob', log_prob)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_type in ['VAE_ind_dec_cond_b', 'VAE_ind_recon_b', 'VAE', 'VAE_ind']:\n",
    "for model_type in ['VAE_ind_recon_b', 'VAE', 'VAE_ind']:\n",
    "    for missingness_type, p in [('dependent', 0.2), ('independent', 0.2)]:\n",
    "        if missingness_type == 'independent':\n",
    "            b_train = np.random.uniform(size=x_train.shape) > p\n",
    "            x_train_ = b_train * x_train\n",
    "            b_valid = np.random.uniform(size=x_valid.shape) > p\n",
    "            x_valid_ = b_valid * x_valid\n",
    "            b_test = np.random.uniform(size=x_test.shape) > p\n",
    "            x_test_ = b_test * x_test\n",
    "        elif missingness_type == 'dependent':\n",
    "            b_train = np.random.uniform(size=x_train.shape) > np.expand_dims(\n",
    "                np.expand_dims(p * (1 + y_train)/10.0, axis=1), axis=2)\n",
    "            x_train_ = b_train * x_train\n",
    "            b_valid = np.random.uniform(size=x_valid.shape) > np.expand_dims(\n",
    "                np.expand_dims(p * (1 + y_valid)/10.0, axis=1), axis=2)\n",
    "            x_valid_ = b_valid * x_valid\n",
    "            b_test = np.random.uniform(size=x_test.shape) > np.expand_dims(\n",
    "                np.expand_dims(p * (1 + y_test)/10.0, axis=1), axis=2)\n",
    "            x_test_ = b_test * x_test\n",
    "\n",
    "        feature_columns = [tf.feature_column.numeric_column(key='x', shape=[mnist_dim, mnist_dim]),\n",
    "                           tf.feature_column.numeric_column(key='b', shape=[mnist_dim, mnist_dim])]\n",
    "\n",
    "        vae = tf.estimator.Estimator(\n",
    "            model_fn=model,\n",
    "            model_dir='mnist_conv_{0}_{1}'.format(model_type, missingness_type),\n",
    "            params={'feature_columns': feature_columns, 'model_type': model_type},\n",
    "            config=tf.estimator.RunConfig(\n",
    "                save_summary_steps=1000,\n",
    "                save_checkpoints_steps=20000,\n",
    "                keep_checkpoint_max=200,\n",
    "                log_step_count_steps=1000))\n",
    "        \n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            {'x': x_train_, 'b': b_train},\n",
    "            shuffle=True)\n",
    "\n",
    "        valid_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            {'x': x_valid_, 'b': b_valid},\n",
    "            shuffle=False)\n",
    "\n",
    "        best_valid_error = None\n",
    "        best_checkpoint = None\n",
    "        best_estimator = None\n",
    "        epochs_since_improvement = 0\n",
    "        for _ in range(max_epochs):\n",
    "            vae.train(steps=None, input_fn=train_input_fn)\n",
    "\n",
    "            eval_result = vae.evaluate(input_fn=train_input_fn)\n",
    "            logging.info('End of epoch evaluation (train set): ' + str(eval_result))\n",
    "\n",
    "            eval_result = vae.evaluate(input_fn=valid_input_fn)\n",
    "            logging.info('End of epoch evaluation (valid set): ' + str(eval_result))\n",
    "\n",
    "            if best_valid_error is None or eval_result['loss'] < best_valid_error:\n",
    "                best_checkpoint = vae.latest_checkpoint()\n",
    "                best_valid_error = eval_result['loss']\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "                if epochs_since_improvement >= max_epoch_without_improvement:\n",
    "                    break\n",
    "\n",
    "        eval_result = vae.evaluate(\n",
    "            input_fn=valid_input_fn,\n",
    "            checkpoint_path=best_checkpoint)\n",
    "        logging.info('Valid set evaluation: {0}'.format(eval_result))\n",
    "        \n",
    "        ### missing data imputation evaluation\n",
    "        \n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn({'x': x_test_, 'b': b_test}, shuffle=False)\n",
    "        x_preds = np.asarray([pred['x'] for pred in vae.predict(input_fn=input_fn,\n",
    "                                                                       checkpoint_path=best_checkpoint)])\n",
    "\n",
    "        x_preds = x_preds.reshape([-1, mnist_dim, mnist_dim])\n",
    "        accuracy = 1.0 - (np.sum(\n",
    "            np.abs(x_test ^ np.reshape(x_preds > 0.5, (np.shape(x_preds)[0], mnist_dim, mnist_dim))))/\n",
    "                          np.prod(x_test.shape))\n",
    "        print('Test set recon accuracy (all data):', accuracy)\n",
    "        accuracy = 1.0 - (np.sum(~b_test *\n",
    "            np.abs(x_test ^ np.reshape(x_preds > 0.5, (np.shape(x_preds)[0], mnist_dim, mnist_dim))))/np.sum(~b_test))\n",
    "        print('Test set recon accuracy (on missing data):', accuracy)\n",
    "        \n",
    "        \n",
    "        ### representation learning evaluation (train linear classifier on learned z)\n",
    "        \n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn({'x': x_train_, 'b': b_train}, shuffle=False)\n",
    "        z_train = np.asarray([pred['z'] for pred in vae.predict(input_fn=input_fn,\n",
    "                                                                       checkpoint_path=best_checkpoint)])\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            {'z': z_train}, y=y_train, shuffle=True)\n",
    "        \n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn({'x': x_valid_, 'b': b_valid}, shuffle=False)\n",
    "        z_valid = np.asarray([pred['z'] for pred in vae.predict(input_fn=input_fn,\n",
    "                                                                       checkpoint_path=best_checkpoint)])\n",
    "        valid_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            {'z': z_valid}, y=y_valid, shuffle=False)\n",
    "        \n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn({'x': x_test_, 'b': b_test}, shuffle=False)\n",
    "        z_test = np.asarray([pred['z'] for pred in vae.predict(input_fn=input_fn,\n",
    "                                                                       checkpoint_path=best_checkpoint)])\n",
    "        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            {'z': z_test}, y=y_test, shuffle=False)\n",
    "        \n",
    "        classifier = tf.estimator.LinearClassifier(\n",
    "            feature_columns=[tf.feature_column.numeric_column(key='z', shape=[z_dim])],\n",
    "            model_dir='mnist_conv_{0}_{1}_classifier'.format(model_type, missingness_type),\n",
    "            n_classes=10,\n",
    "            config=tf.estimator.RunConfig(\n",
    "                save_summary_steps=1000,\n",
    "                save_checkpoints_steps=6000,\n",
    "                keep_checkpoint_max=1000,\n",
    "                log_step_count_steps=1000))\n",
    "        \n",
    "        best_valid_error = None\n",
    "        best_checkpoint = None\n",
    "        best_estimator = None\n",
    "        epochs_since_improvement = 0\n",
    "        for _ in range(max_epochs):\n",
    "            classifier.train(steps=None, input_fn=train_input_fn)\n",
    "\n",
    "            eval_result = classifier.evaluate(input_fn=train_input_fn)\n",
    "            logging.info('End of epoch evaluation (train set): ' + str(eval_result))\n",
    "\n",
    "            eval_result = classifier.evaluate(input_fn=valid_input_fn)\n",
    "            logging.info('End of epoch evaluation (valid set): ' + str(eval_result))\n",
    "\n",
    "            if best_valid_error is None or eval_result['loss'] < best_valid_error:\n",
    "                best_checkpoint = classifier.latest_checkpoint()\n",
    "                best_valid_error = eval_result['loss']\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "                if epochs_since_improvement >= max_epoch_without_improvement:\n",
    "                    break\n",
    "\n",
    "        eval_result = classifier.evaluate(\n",
    "            input_fn=valid_input_fn,\n",
    "            checkpoint_path=best_checkpoint)\n",
    "        logging.info('Valid set evaluation: {0}'.format(eval_result))\n",
    "        \n",
    "        eval_result = classifier.evaluate(\n",
    "            input_fn=test_input_fn,\n",
    "            checkpoint_path=best_checkpoint)\n",
    "        logging.info('Test set evaluation: {0}'.format(eval_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
